"""
NanoGenesis ä¸»ç±» - æ•´åˆæ‰€æœ‰æ ¸å¿ƒç»„ä»¶ (Unified)
"""

import sys
import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List
import logging
import json
import asyncio

sys.path.insert(0, str(Path(__file__).parent.parent))

from genesis.core.base import PerformanceMetrics, Message, MessageRole
from genesis.core.registry import ToolRegistry
from genesis.core.loop import AgentLoop
from genesis.core.context import SimpleContextBuilder # Type hinting
from genesis.memory import SQLiteMemoryStore, SessionManager  # Type hinting
from genesis.core.mission import MissionManager # Type hinting
from genesis.core.cognition import CognitiveProcessor # Type hinting
from genesis.core.scheduler import AgencyScheduler # Type hinting
from genesis.core.provider_manager import ProviderRouter # Type hinting

from genesis.core.config import config
from genesis.core.trust_anchor import TrustAnchorManager

logger = logging.getLogger(__name__)


class NanoGenesis:
    """
    NanoGenesis - è‡ªè¿›åŒ–çš„è½»é‡çº§æ™ºèƒ½ Agent (å•è„‘æ¶æ„ç‰ˆ)
    Genesis Core 2.0 Refactored
    """
    
    def __init__(
        self,
        user_id: str,
        config: Any,
        tools: ToolRegistry,
        context: Any,
        provider_router: Any,
        memory: SQLiteMemoryStore,
        session_manager: SessionManager,
        mission_manager: MissionManager, # Injected
        scheduler: AgencyScheduler,
        cognition: CognitiveProcessor,
        trust_anchor: TrustAnchorManager,
        optimization_components: Dict[str, Any] = None,
        max_iterations: int = 10,
        enable_optimization: bool = True
    ):
        """
        åˆå§‹åŒ– (Dependency Injection)
        Use GenesisFactory to create instances.
        """
        self.user_id = user_id
        self.config = config
        self.tools = tools
        
        # --- PHASE 3: VISUAL CORTEX ---
        # Auto-register VisualTool if not present
        from genesis.tools.visual_tool import VisualTool
        if "visual" not in self.tools:
            self.tools.register(VisualTool())
        
        self.context = context
        self.provider_router = provider_router
        self.cloud_provider = provider_router # Compatibility
        self.memory = memory
        self.session_manager = session_manager
        self.mission_manager = mission_manager
        self.scheduler = scheduler
        self.cognition = cognition
        self.trust_anchor = trust_anchor
        self.max_iterations = max_iterations
        self.enable_optimization = enable_optimization
        
        # Unpack Optimization Components
        self.optimization_components = optimization_components or {}
        self.prompt_optimizer = self.optimization_components.get('prompt_optimizer')
        self.behavior_optimizer = self.optimization_components.get('behavior_optimizer')
        self.tool_optimizer = self.optimization_components.get('tool_optimizer')
        self.profile_evolution = self.optimization_components.get('profile_evolution')
        self.adaptive_learner = self.optimization_components.get('adaptive_learner')

        # Wire up Decision Transparency callback
        self.loop = AgentLoop(
            tools=self.tools,
            context=self.context,
            provider=self.cloud_provider,
            max_iterations=max_iterations
        )
        self.loop.on_tool_call = self._log_tool_reason
        
        # æ€§èƒ½ç›‘æ§ & æ—¥å¿—
        self.metrics_history = []
        self.reasoning_log: list = []
        
        logger.debug(f"âœ“ NanoGenesis 2.0 Agent Assembled")
        self._history_loaded = False
    
    async def process(
        self,
        user_input: str,
        user_context: Optional[str] = None,
        problem_type: str = "general",
        step_callback: Optional[Any] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆå•è„‘æ¶æ„ + çº¯ç²¹å…ƒè®¤çŸ¥åè®® + æµå¼æ‰§è¡Œï¼‰
        
        Refactored for "Body Swap":
        - ç§»é™¤ [ACQUISITION_PLAN] ç­‰ç¡¬ç¼–ç åˆ†æ”¯
        - å¯ç”¨ AdaptiveLearner è‡ªé€‚åº”
        - å¯ç”¨ Loop ç›´æ¥å·¥å…·è°ƒç”¨
        """
        import time
        process_start_time = time.time()
        
        # Load compressed history (Long-term memory)
        if hasattr(self, "_history_loaded") and not self._history_loaded:
             if hasattr(self.context, "load_compressed_history"):
                 await self.context.load_compressed_history()
             self._history_loaded = True
        
        # 0. å‡†å¤‡åŸºåº• Prompt (Persona)
        base_prompt = ""
        if self.adaptive_learner:
            base_prompt = self.adaptive_learner.generate_adaptive_prompt()
        else:
            base_prompt = self.context.system_prompt
            
        if self.context.get_history_length() == 0:
            if await self.session_manager.restore_last_session():
                history = await self.session_manager.get_full_history()
                for item in history:
                    role = MessageRole.USER if item['role'] == 'user' else MessageRole.ASSISTANT
                    # Skip tool outputs for now or handle them if needed, mostly we need the conversation flow
                    self.context.add_to_history(Message(role=role, content=item['content']))
                logger.info(f"ğŸ§  Session Hydrated: {len(history)} turns loaded from SQLite")
            else:
                 logger.info(f"âœ¨ New Session Started: {self.session_manager.session_id}")

        # --- Genesis Triad Pipeline ---
        
        # --- Ouroboros Loop (The Navigator) ---
        MAX_RETRIES = 3
        MAX_DAISY_CHAIN = 5
        execution_history = []
        final_response = None
        final_metrics = None
        final_success = False
        final_optimization_info = {}

        loop_count = 0
        error_count = 0
        current_input = user_input
        accumulated_response = ""
        
        while loop_count < MAX_DAISY_CHAIN:
            loop_count += 1
            logger.info(f"ğŸ”„ Ouroboros Loop: Iteration {loop_count} (Errors: {error_count})")
            
            # 1. æ´å¯Ÿé˜¶æ®µ (Awareness Phase)
            if loop_count == 1:
                # é€‰é¡¹æ„ŸçŸ¥æ‰©å±•ï¼šå½“ç”¨æˆ·è¾“å…¥æ˜¯å•å­—æ¯/æ•°å­—æ—¶ï¼Œå°è¯•è¿˜åŸå…¶é€‰é¡¹ä¸Šä¸‹æ–‡
                awareness_input = self._expand_option_input(current_input)
                
                # --- Entity B: Context Packager Phase ---
                # Packager uses read-only tools to scan the environment and gather precise context
                # so the Executor doesn't have to waste tokens probing blindly.
                from genesis.core.packager import ContextPackager
                packager = ContextPackager(self.cloud_provider)
                logger.info("ğŸ“¦ Packager Phase: Scouting environment...")
                
                mission_payload = await packager.build_payload(awareness_input, step_callback)
                logger.info(f"ğŸ“¦ Payload Generated ({len(mission_payload)} chars)")
                
                # å–æœ€è¿‘ 3 æ¡å¯¹è¯å†å²ï¼Œä½œä¸º Oracle çš„ä¸Šä¸‹æ–‡é”šç‚¹
                # è§£å†³ awareness_phase ä¸ºå­¤ç«‹ LLM è°ƒç”¨ã€æ— æ³•æ„ŸçŸ¥ä¸Šè½®å¯¹è¯çš„é—®é¢˜
                _recent_ctx = []
                try:
                    if hasattr(self, 'context') and self.context:
                        _hist = self.context.get_history()
                        for _m in _hist[-6:]:  # æœ€è¿‘ 6 æ¡ï¼ˆ3è½®å¯¹è¯ï¼‰
                            _role = getattr(_m, 'role', None) or (_m.get('role') if isinstance(_m, dict) else None)
                            _content = getattr(_m, 'content', None) or (_m.get('content') if isinstance(_m, dict) else None)
                            if _role in ('user', 'assistant') and _content:
                                _recent_ctx.append({'role': _role, 'content': str(_content)})
                except Exception:
                    pass
                
                # Delegate to Cognitive Processor
                oracle_output = await self.cognition.awareness_phase(awareness_input, recent_context=_recent_ctx)
                self.reasoning_log.append({
                    "timestamp": time.time(),
                    "stage": "AWARENESS",
                    "content": oracle_output
                })
                logger.info(f"âœ“ æ´å¯Ÿå®Œæˆ: {oracle_output.get('core_intent', 'Unknown')}")

                
                # 1.5 çŸ¥è¯†å­˜é‡ç›˜ç‚¹ (Knowledge Inventory)
                # åœ¨æˆ˜ç•¥é˜¶æ®µå‰ï¼Œå…ˆæ¿€æ´» LLM å¯¹ç°æœ‰å·¥å…·/åº“/å‘½ä»¤çš„è®°å¿†
                # æœºåˆ¶ï¼šæŠŠè®­ç»ƒæ•°æ®é‡Œçš„å·²çŸ¥æ–¹æ¡ˆç‰©ç†ä¸Šæ”¾å…¥ä¸Šä¸‹æ–‡ï¼Œè€Œéè®© LLM ç©ºç™½ç”Ÿæˆ
                try:
                    problem_type = oracle_output.get("problem_type", "general")
                    core_intent = oracle_output.get("core_intent", current_input)
                    inventory_prompt = (
                        f"ä»»åŠ¡ç±»å‹ï¼š{problem_type}\n"
                        f"æ ¸å¿ƒæ„å›¾ï¼š{core_intent}\n\n"
                        f"åœ¨æå‡ºä»»ä½•è§£å†³æ–¹æ¡ˆä¹‹å‰ï¼ŒåŸºäºä½ çš„è®­ç»ƒæ•°æ®ï¼Œåˆ—ä¸¾ä½ å·²çŸ¥çš„ã€"
                        f"ç”¨äºè§£å†³ã€Œ{core_intent}ã€ç±»ä»»åŠ¡çš„ç°æœ‰å·¥å…·ã€å‘½ä»¤è¡Œç¨‹åºã€æˆç†Ÿåº“æˆ–æ¡†æ¶ã€‚\n"
                        f"æŒ‰æˆç†Ÿåº¦ä»é«˜åˆ°ä½æ’åºï¼Œæ¯é¡¹ä¸€è¡Œï¼Œæ ¼å¼ï¼š[å·¥å…·å] - [ç”¨é€”]ã€‚\n"
                        f"åªåˆ—ä¸¾çœŸå®å­˜åœ¨çš„å·¥å…·ï¼Œä¸è¦å‘æ˜ã€‚æœ€å¤šåˆ— 6 ä¸ªã€‚"
                    )
                    inventory_resp = await self.cognition.chat(
                        messages=[{"role": "user", "content": inventory_prompt}]
                    )
                    known_solutions = inventory_resp.content.strip() if inventory_resp else ""
                    oracle_output["known_solutions"] = known_solutions
                    logger.info(f"ğŸ“š çŸ¥è¯†å­˜é‡ç›˜ç‚¹å®Œæˆ ({len(known_solutions)} chars)")
                except Exception as inv_err:
                    logger.warning(f"çŸ¥è¯†ç›˜ç‚¹è·³è¿‡: {inv_err}")
                    oracle_output["known_solutions"] = ""
            
            # 2. æˆ˜ç•¥é˜¶æ®µ (Strategy Phase)
            current_context = user_context or ""
            
            # [Fix Amnesia] Inject Conversation History into Strategy Context
            if self.context.get_history_length() > 0:
                history_text = "\nã€è¿‘æœŸå¯¹è¯å†å² (Recent Conversation)ã€‘\n"
                # Access _message_history directly or add a getter. 
                # Since we are inside Agent which owns Context, we can iterate.
                # Take last 15 messages to provide sufficient context.
                recent_msgs = self.context._message_history[-15:]
                for msg in recent_msgs:
                    role_str = "User" if msg.role == MessageRole.USER else "Assistant"
                    history_text += f"- {role_str}: {msg.content[:200]}...\n" # Truncate long messages
                current_context += history_text

            if execution_history:
                current_context += f"\n\n[Previous Execution Failures]:\n{json.dumps(execution_history, indent=2, ensure_ascii=False)}"
            
            # Inject Mission Payload from Entity B into the Executor's context
            if loop_count == 1 and 'mission_payload' in locals():
                current_context += f"\n\n[ğŸ“¦ Context Packager Payload (Entity B)]\n{mission_payload}\n"
            
            # æ³¨å…¥çŸ¥è¯†å­˜é‡ç›˜ç‚¹ç»“æœï¼ˆè®© strategy_phase çœ‹åˆ°å·²çŸ¥å·¥å…·æ¸…å•ï¼Œå†ç”Ÿæˆæ–¹æ¡ˆï¼‰
            known_solutions = oracle_output.get("known_solutions", "")
            if known_solutions:
                current_context += (
                    f"\n\n[ğŸ“š çŸ¥è¯†å­˜é‡é”šç‚¹ - å·²çŸ¥ç°æœ‰å·¥å…·/æ–¹æ¡ˆ (ç”±çŸ¥è¯†åº“æ£€ç´¢)]\n"
                    f"{known_solutions}\n"
                    f"â†’ ä¼˜å…ˆä»ä»¥ä¸Šå·²æœ‰å·¥å…·ä¸­é€‰æ‹©ï¼Œéå¿…è¦ä¸ä»é›¶ç¼–å†™ä»£ç ã€‚"
                )
            
            # --- PRUDENT COGNITION (Perception Layer) ---
            # Phase 1.5: World Model Snapshot
            # Run fast local checks before planning
            from genesis.core.capability import CapabilityScanner
            from genesis.core.entropy import EntropyMonitor
            
            # Lazy init monitor with higher tolerance (Relaxed Mode)
            if not hasattr(self, 'entropy_monitor'):
                self.entropy_monitor = EntropyMonitor(window_size=6)
            else:
                # æ¯æ¬¡æ–°å¯¹è¯å¼€å§‹æ—¶é‡ç½®ï¼Œé¿å…æºå¸¦ä¸Šæ¬¡ä¼šè¯çš„å†å²è¯¯è§¦å‘ stagnant
                self.entropy_monitor.reset()

                
            world_model_snapshot = CapabilityScanner.scan()
            
            # --- ENTROPY CHECK (Loop Breaker) ---
            # Capture current state vector: (LastOutput + CWD + ActiveMission)
            # Use oracle_output or last_tool_result as proxy for output state
            last_tool_out = str(oracle_output) # Using Oracle's view of recent history
            cwd = world_model_snapshot.get('cwd', 'unknown') # CapabilityScanner doesn't return CWD yet? 
            # Actually, let's use a simpler proxy: The concatenation of recent tool outputs from history
            recent_tool_out = ""
            if execution_history:
                 recent_tool_out = str(execution_history[-1])
            
            active_mission_id = "root"
            if hasattr(self, 'mission_manager'):
                 am = self.mission_manager.get_active_mission()
                 if am: active_mission_id = am.id
                 
            self.entropy_monitor.capture(recent_tool_out, cwd, active_mission_id)
            
            # --- DYNAMIC ENTROPY (Soft Interrupt) ---
            # Instead of returning, we gather the signal and inject it into cognition
            entropy_analysis = self.entropy_monitor.analyze_entropy()
            if entropy_analysis.get('status') == 'stagnant':
                logger.warning(f"âš ï¸ High Entropy Detected: {entropy_analysis}")

            # --- MISSION CONTEXT TREE INTEGRATION ---
            # Ensure Active Mission Exists
            mission_lineage = []
            active_mission = None
            if hasattr(self, 'mission_manager'):
                active_mission = self.mission_manager.get_active_mission()
                if not active_mission:
                    # Auto-create Root Mission for current intent
                    core_intent = oracle_output.get("core_intent", "General Task")
                    active_mission = self.mission_manager.create_mission(core_intent)
                    logger.info(f"ğŸŒ± Created New Root Mission: {active_mission.objective}")
                
                mission_lineage = self.mission_manager.get_mission_lineage(active_mission.id)

            # Retrieve Active Jobs (Async System)
            active_jobs = []
            if hasattr(self, 'tools'):
                shell_tool = self.tools.get('shell')
                if shell_tool and hasattr(shell_tool, 'job_manager') and shell_tool.job_manager:
                    active_jobs = shell_tool.job_manager.list_jobs(active_only=True)

            # Delegate to Cognitive Processor
            if step_callback:
                if asyncio.iscoroutinefunction(step_callback):
                    await step_callback("strategy", "åˆ¶å®šæˆ˜ç•¥...")
                else:
                    step_callback("strategy", "åˆ¶å®šæˆ˜ç•¥...")
            strategic_blueprint = await self.cognition.strategy_phase(
                user_input=current_input, 
                oracle_output=oracle_output, 
                user_context=current_context,
                adaptive_stats=self.adaptive_learner.get_stats() if self.adaptive_learner else {},
                active_mission=active_mission,
                mission_lineage=mission_lineage,
                world_model=world_model_snapshot, # Inject World Model
                active_jobs=active_jobs, # Inject Active Jobs
                entropy_analysis=entropy_analysis # Inject Entropy Signal
            )
            
            self.reasoning_log.append({
                "timestamp": time.time(),
                "stage": "STRATEGY",
                "content": strategic_blueprint
            })
            
            # --- Decode strategy using the Protocol Decoder (No hardcoded strings!) ---
            from genesis.intelligence.protocol_decoder import ProtocolDecoder
            intent_type, intent_content, _ = ProtocolDecoder.decode_strategy(strategic_blueprint)

            # --- Clarification Protocol (The Short Circuit) ---
            if intent_type == "clarification":
                logger.info("âš ï¸ æˆ˜ç•¥é˜¶æ®µè¯·æ±‚æ¾„æ¸…ï¼Œä¸­æ–­æ‰§è¡Œ")
                return {
                    'response': accumulated_response + ("\n\n" if accumulated_response else "") + intent_content, 
                    'metrics': None,
                    'success': True, 
                    'optimization_info': {'status': 'clarification_requested'}
                }

            # --- 3D Mission Tree: Capability Forge (The Z-Axis Jump) ---
            if intent_type == "forge":
                logger.warning("ğŸ”¨ [è‡ªè¿›åŒ–è§¦å‘] æˆ˜ç•¥é˜¶æ®µåˆ¤å®šç¼ºå°‘å…³é”®èƒ½åŠ›ï¼Œå¯åŠ¨ Z è½´åˆ†æ”¯ (Capability Forge)")
                
                if hasattr(self, 'mission_manager') and active_mission:
                    # æ´¾ç”Ÿå­ä»»åŠ¡ (Zè½´)
                    forge_mission = self.mission_manager.create_mission(
                        objective=f"[FORGE] è·å–æ–°èƒ½åŠ›: \n{intent_content}",
                        parent_id=active_mission.id
                    )
                    active_mission = forge_mission
                    logger.info(f"ğŸŒ¿ æˆåŠŸæŠ˜å ä¸»çº¿ï¼Œæ´¾ç”Ÿèƒ½åŠ›é”»é€ å­ä»»åŠ¡: {forge_mission.id}")
                
                # å‘ŠçŸ¥æœ¬è½®çš„ A (Executor) ç«‹å³å»æ‰“é€ è¿™ä¸ªå·¥å…·
                current_input = (
                    f"CRITICAL OVERRIDE - CAPABILITY FORGE REQUIRED.\n"
                    f"You must acquire or create a new tool to proceed.\n"
                    f"Forge Details:\n{intent_content}\n"
                    f"Action Required: Use `skill_creator` to write the script OR `github_skill_search` to find it."
                )
                
                # We log it, but do not exit. We let the loop run the forge task.
                accumulated_response += f"\n\n[CAPABILITY_FORGE_INITIATED]\n{intent_content}\n\n"
                
                # Skip the standard loop metrics tracking for this purely internal phase jump
                # (Or let it run normally so the executor handles it). We let it run normally!
                logger.info("âœ“ æˆ˜ç•¥è“å›¾å·²å˜è½¨ä¸ºèƒ½åŠ›é”»é€ æŒ‡ä»¤")
            else:
                logger.info("âœ“ æˆ˜ç•¥è“å›¾å·²ç”Ÿæˆ")
            
            # --- å†³ç­–æ—¥å¿—ï¼šè®°å½•æœ¬è½®é”šç‚¹é€‰æ‹© ---
            _decision_id = None
            try:
                if hasattr(self, 'mission_manager') and active_mission:
                    _anchor_options = []
                    _known = oracle_output.get("known_solutions", "")
                    if _known:
                        # ä»çŸ¥è¯†å­˜é‡ç›˜ç‚¹é‡Œæå–å·¥å…·åï¼ˆæ¯è¡Œæ ¼å¼ "[å·¥å…·å] - ç”¨é€”"ï¼‰
                        _anchor_options = [
                            line.strip().split(" - ")[0].strip("[]").strip()
                            for line in _known.splitlines()
                            if line.strip() and " - " in line
                        ][:6]
                    _chosen = strategic_blueprint[:150].replace("\n", " ")
                    _problem_type = oracle_output.get("problem_type", "general")
                    _decision_id = self.mission_manager.log_decision(
                        mission_id=active_mission.id,
                        problem_type=_problem_type,
                        anchor_options=_anchor_options,
                        chosen_anchor=_chosen,
                    )
                    logger.debug(f"ğŸ“‹ å†³ç­–å·²è®°å½• (id={_decision_id}, type={_problem_type})")
            except Exception as _dl_err:
                logger.debug(f"å†³ç­–æ—¥å¿—è·³è¿‡: {_dl_err}")
            

            # ç§»é™¤æ¯æ‰ç¼“å­˜çš„ ContextualPromptFilter (å®ƒä¼šæ‰“ä¹±æ®µè½ï¼Œç ´å Prefix Hash)
            # ç§»é™¤å¯¹ system_prompt çš„åŠ¨æ€æ³¨å…¥ï¼Œä¿æŒ system æ¶ˆæ¯ä»ä¸€å¼€å§‹åˆ°æœ€åéƒ½æ˜¯é™æ€çš„ï¼
            
            # ç»„åˆæ‰€æœ‰çš„åŠ¨æ€ä¸Šä¸‹æ–‡ï¼Œå¡ç»™æœ€æ–°çš„ä¸€æ¡ user_contextï¼Œå€Ÿæ­¤æ¥ä¿æŒç³»ç»Ÿæç¤ºè¯å¹²å‡€ä¸”å¼ºå‘½ä¸­ã€‚
            final_user_context = f"{user_context}\n\n[æˆ˜ç•¥è“å›¾]\n{strategic_blueprint}"
            if _decision_id:
                final_user_context += f"\n[è®°å½•å†³ç­–ID: {_decision_id} ({_problem_type})]"
                
            self.loop.provider = self.cloud_provider
            
            try:
                # è®© Loop å¤„ç†æ€è€ƒã€è°ƒç”¨å·¥å…·ã€å†æ€è€ƒã€æœ€ç»ˆå›å¤
                response, metrics = await self.loop.run(
                    user_input=current_input,
                    step_callback=step_callback,
                    user_context=final_user_context,
                    raw_memory=oracle_output.get("memory_pull", []),
                    **kwargs
                )
                
                final_metrics = metrics
                final_success = metrics.success
                final_optimization_info = self._check_and_optimize() if self.enable_optimization else {}
                
                if metrics.success:
                    # 0. Check for Cognitive Escalation (Strategic Interrupt)
                    exec_intent_type, clean_msg, _ = ProtocolDecoder.decode_execution(response)
                    if exec_intent_type == "interrupt":
                        logger.warning(f"ğŸ”„ Strategic Interrupt Received: {clean_msg}")
                        # Force loop back to Strategy Phase
                        # Clean signal for context
                        execution_history.append(f"ESCALATION: {clean_msg}")
                        accumulated_response += f"\n\n[ESCALATION] {clean_msg}\n\n"
                        error_count = 0 # Reset error count as this is a controlled escalation
                        continue
                    
                    # 1. Check Tool Calls (Preferred)
                    next_instruction = None
                    if metrics.tool_calls:
                        for tc in metrics.tool_calls:
                            if tc['function']['name'] == 'chain_next':
                                try:
                                    args = json.loads(tc['function']['arguments'])
                                    next_instruction = args.get('instruction')
                                    logger.info(f"ğŸ”— Daisy Chain Triggered via Tool: {next_instruction}")
                                    break
                                except:
                                    pass
                    
                    # 2. Check Text Regex (Legacy Fallback)
                    if not next_instruction and ">> NEXT:" in response:
                        import re
                        match = re.search(r">> NEXT:\s*(.+)", response)
                        if match:
                            next_instruction = match.group(1).strip()
                            logger.info(f"ğŸ”— Daisy Chain Triggered via Text: {next_instruction}")

                    if next_instruction:
                        # Append to accumulator
                        accumulated_response += response + "\n\n---\n\n"
                        
                        # Update Input for next loop
                        current_input = next_instruction
                        execution_history = [] # Clear history as previous step succeeded
                        error_count = 0 # Reset error count for new task
                        continue
                    
                    # Mission Accomplished (No chain)
                    accumulated_response += response
                    logger.info("âœ“ Ouroboros Loop: Mission Accomplished")
                    # å†³ç­–æ—¥å¿—ï¼šæ ‡è®°æˆåŠŸ
                    if _decision_id is not None and hasattr(self, 'mission_manager'):
                        try: self.mission_manager.update_decision_outcome(_decision_id, 'success')
                        except: pass
                    break
                    
                else:
                    error_count += 1
                    logger.warning(f"âš ï¸ Ouroboros Loop: Error Attempt {error_count} Failed.")
                    error_entry = f"Attempt {error_count} Failed. Last output: {response[-200:] if response else 'No response'}"
                    execution_history.append(error_entry)
                    
                    if error_count >= MAX_RETRIES:
                        # --- MISSION TREE BACKTRACKING ---
                        # ä¼˜å…ˆå°è¯•ä»»åŠ¡æ ‘å›æº¯ï¼Œè€Œéç›´æ¥ç»ˆæ­¢
                        backtracked = False
                        if response and "[STRATEGIC_INTERRUPT]" in response:
                            interrupt_detail = response.replace("[STRATEGIC_INTERRUPT]", "").strip()
                            
                            # å°è¯•ä»ä»»åŠ¡æ ‘çˆ¬å›çˆ¶èŠ‚ç‚¹
                            if hasattr(self, 'mission_manager'):
                                active_mission = self.mission_manager.get_active_mission()
                                if active_mission and active_mission.parent_id:
                                    logger.warning(f"ğŸ”„ ä»»åŠ¡æ ‘å›æº¯ï¼šä» '{active_mission.objective[:50]}' çˆ¬å›çˆ¶èŠ‚ç‚¹...")
                                    parent_mission = self.mission_manager.backtrack_to_parent(
                                        active_mission.id,
                                        error_summary=interrupt_detail
                                    )
                                    if parent_mission:
                                        # è·å–æ‰€æœ‰å·²å¤±è´¥å­è·¯å¾„ï¼ˆè®©ä¸‹æ¬¡ strategy æ’é™¤å®ƒä»¬ï¼‰
                                        failed_paths = self.mission_manager.get_failed_children(parent_mission.id)
                                        failed_hint = ""
                                        if failed_paths:
                                            failed_hint = (
                                                f"\n\n[BACKTRACK CONTEXT] ä»¥ä¸‹è·¯å¾„å·²å°è¯•å¹¶å¤±è´¥ï¼Œç¦æ­¢å†æ¬¡é€‰æ‹©ï¼š\n"
                                                + "\n".join(f"- {p}" for p in failed_paths)
                                            )
                                        
                                        # å†³ç­–æ—¥å¿—ï¼šæ ‡è®°å›æº¯ï¼ˆè¿™æ˜¯æ·±åº¦åæ€çš„é”šç‚¹äº‹ä»¶ï¼‰
                                        if _decision_id is not None:
                                            try: self.mission_manager.update_decision_outcome(_decision_id, 'backtracked')
                                            except: pass
                                        # è§¦å‘é”šç‚¹çº§æ·±åº¦åæ€ï¼ˆéé˜»å¡ï¼‰
                                        try:
                                            adaptive = self.optimization_components.get('adaptive_learner') if hasattr(self, 'optimization_components') else None
                                            if adaptive and hasattr(adaptive, 'trigger_anchor_reflection'):
                                                recent_decisions = self.mission_manager.get_recent_decisions(limit=15)
                                                asyncio.create_task(
                                                    adaptive.trigger_anchor_reflection(
                                                        llm_chat_fn=self.cognition.chat,
                                                        decisions=recent_decisions,
                                                    )
                                                )
                                        except Exception as _ar_err:
                                            logger.debug(f"é”šç‚¹åæ€è·³è¿‡: {_ar_err}")
                                        
                                        # é‡ç½®å¾ªç¯çŠ¶æ€ï¼Œç”¨çˆ¶ä»»åŠ¡ç›®æ ‡é‡è¯•
                                        error_count = 0
                                        current_input = parent_mission.objective + failed_hint
                                        execution_history = []
                                        logger.info(f"â†©ï¸ å›æº¯æˆåŠŸï¼Œé‡è¯•çˆ¶ä»»åŠ¡: {parent_mission.objective[:50]}")
                                        backtracked = True
                        
                        if not backtracked:
                            # --- AUTO-DEBRIEF: æ— æ³•å›æº¯ï¼ˆæ ¹èŠ‚ç‚¹ï¼‰ï¼Œä¸»åŠ¨æ±‡æŠ¥åè®® ---
                            if response and "[STRATEGIC_INTERRUPT]" in response:
                                logger.warning("ğŸ”” AUTO-DEBRIEF: æ ¹èŠ‚ç‚¹ä¸­æ–­ï¼Œç”Ÿæˆç”¨æˆ·è¯´æ˜")
                                interrupt_detail = response.replace("[STRATEGIC_INTERRUPT]", "").strip()
                                accumulated_response += (
                                    f"âš ï¸ **æ‰§è¡Œè¢«è‡ªåŠ¨ç†”æ–­ä¸­æ–­**\n\n"
                                    f"**å‘ç”Ÿäº†ä»€ä¹ˆ**ï¼šæˆ‘åœ¨å°è¯•æ‰§è¡Œä»»åŠ¡æ—¶è§¦å‘äº†å®‰å…¨ç†”æ–­æœºåˆ¶ã€‚å…·ä½“åŸå› ï¼š{interrupt_detail}\n\n"
                                    f"**ä¸ºä»€ä¹ˆåœä¸‹æ¥**ï¼šä¸ºäº†é¿å…é™·å…¥æ— æ„ä¹‰çš„é‡å¤å¾ªç¯ã€æ¶ˆè€—æ›´å¤šèµ„æºï¼Œç³»ç»Ÿä¸»åŠ¨ä¸­æ–­äº†æœ¬æ¬¡æ‰§è¡Œã€‚\n\n"
                                    f"**æ¥ä¸‹æ¥æ€ä¹ˆåŠ**ï¼š\n"
                                    f"1. å¦‚æœæ˜¯å·¥å…·è¿ç»­å¤±è´¥ï¼ˆå¦‚æƒé™ä¸è¶³ã€ç¯å¢ƒé—®é¢˜ï¼‰ï¼Œè¯·å‘ŠçŸ¥æˆ‘æ¢ä¸€ç§æ–¹æ³•ï¼Œæˆ–è€…æˆäºˆå¿…è¦æƒé™ã€‚\n"
                                    f"2. å¦‚æœæ˜¯ç­–ç•¥é—®é¢˜ï¼Œæˆ‘å¯ä»¥é‡æ–°åˆ¶å®šæ‰§è¡Œæ–¹æ¡ˆã€‚\n"
                                    f"3. æ‚¨å¯ä»¥ç›´æ¥å‘Šè¯‰æˆ‘å¦‚ä½•ç»§ç»­ï¼Œæˆ‘ä¼šç«‹å³é‡å¯æ‰§è¡Œã€‚"
                                )
                            else:
                                accumulated_response += response or "Error: Max retries exceeded."
                            logger.error("âŒ Ouroboros Loop: Max Retries Exceeded.")
                            break
            
            except Exception as e:
                logger.error(f"å¤§è„‘æ‰§è¡Œä¸¥é‡å¤±è´¥: {e}")
                execution_history.append(str(e))
                error_count += 1
                if error_count >= MAX_RETRIES:
                    return self._error_response(f"System Critical: Cloud Brain Failure - {e}")

        # --- Phase 3: The Packager (Conscious Wrapper) ---
        # The Stateless Executor has finished. Now we wake up the conversational brain
        # to look at what the executor did, and package it into a nice response for the user.
        logger.info("ğŸ“¦ Entering Packager Phase: Generating conversational response based on raw execution data.")
        
        packager_prompt = (
            "You are the Conversational Packager for NanoGenesis.\n"
            "An unconscious, stateless tool executor has just run a series of actions based on the user's request.\n"
            f"User Original Request: {current_input}\n"
            "--------------------\n"
            "Raw Execution Trace:\n"
            f"{accumulated_response}\n"
            "--------------------\n"
            "Your Task: Read the raw trace above. If the executor succeeded, tell the user what was done in a natural, polite way. "
            "If the executor failed (e.g. error, missing tools), explain to the user what went wrong and ask how they want to proceed. "
            "DO NOT HALLUCINATE TOOL CALLS OR ACTIONS THAT ARE NOT IN THE TRACE."
        )
        
        try:
            # We use cognition.chat to act as the packager, injecting the current conversation memory
            # so it remembers the user profile and previous chats.
            packager_messages = []
            if hasattr(self, 'context') and self.context:
                if hasattr(self.context, 'system_prompt') and self.context.system_prompt:
                     packager_messages.append({'role': 'system', 'content': self.context.system_prompt})
                     
                _hist = self.context.get_history()
                for _m in _hist[-10:]:
                    _role = getattr(_m, 'role', None) or (_m.get('role') if isinstance(_m, dict) else None)
                    _content = getattr(_m, 'content', None) or (_m.get('content') if isinstance(_m, dict) else None)
                    if _role in ('user', 'assistant') and _content:
                         packager_messages.append({'role': _role, 'content': str(_content)})
                         
            packager_messages.append({'role': 'user', 'content': packager_prompt})
            
            if step_callback:
                if asyncio.iscoroutinefunction(step_callback):
                    await step_callback("strategy", "æ­£åœ¨æ•´ç†æ‰§è¡Œç»“æœå¹¶ç»„ç»‡è¯­è¨€å›ç­”...")
                else:
                    step_callback("strategy", "æ­£åœ¨æ•´ç†æ‰§è¡Œç»“æœå¹¶ç»„ç»‡è¯­è¨€å›ç­”...")
                    
            packager_response = await self.cognition.chat(messages=packager_messages)
            packaged_output = packager_response.content
        except Exception as e:
            logger.warning(f"Packager failed to format response: {e}")
            # Fallback to the raw trace if the packager fails
            packaged_output = "ã€æ‰§è¡Œè·Ÿè¸ªæ—¥å¿—ã€‘\n" + accumulated_response

        # --- Memory Update: Append current turn to session context ---
        # å¿…é¡»æ‰‹åŠ¨å›å†™åˆ° contextï¼Œå¦åˆ™ä¸‹ä¸€è½®å¯¹è¯ä¼šä¸¢å¤±ä¸Šä¸‹æ–‡
        self.context.add_to_history(Message(role=MessageRole.USER, content=user_input))
        self.context.add_to_history(Message(role=MessageRole.ASSISTANT, content=packaged_output))

        # Re-assign response to packaged_output for the rest of the flow
        response = packaged_output

        # 3. è®°å½•ä¸å­¦ä¹  (The Evolution)
        optimization_info = {}
        
        # --- NEW PHASE: Cognitive Extraction (Sub-Agent insights) & The Handshake Protocol ---
        import re
        extracted_insight = None
        for msg in self.context._message_history:
            # Look for the new async sub-agent reflection format
            # Or the old OPERATIONAL_METRICS format for backwards compatibility
            if msg.role == MessageRole.TOOL and isinstance(msg.content, str):
                if "<reflection>" in msg.content or "<OPERATIONAL_METRICS>" in msg.content:
                    # Look for anything resembling a cognitive insight, wisdom, or summary
                    # Since we relaxed the YAML rule, we use a broader heuristic or just capture the essence
                    # For now, let's catch the specific marker if the sub-agent adhered to it
                    insight_match = re.search(r"(?:cognitive_insight|insight|è§„å¾‹):?\s*([^\n]+)", msg.content, re.IGNORECASE)
                    if insight_match:
                        extracted_insight = insight_match.group(1).strip()
                        
        if extracted_insight and self.adaptive_learner:
            logger.info(f"ğŸ§  Cognitive Insight Detected (Waiting for Handshake): {extracted_insight}")
            # Do NOT add it automatically. Initiate Handshake Protocol!
            handshake_msg = (
                f"\n\n---\n"
                f"ğŸ¤ **ã€ç³»ç»Ÿä¼˜åŒ–æ¡æ‰‹è¯·æ±‚ã€‘**\n"
                f"åœ¨åˆšåˆšçš„åå°æ¢é’ˆä»»åŠ¡ä¸­ï¼Œå­ä»£ç†æ€»ç»“å‡ºäº†ä¸€æ¡å¯èƒ½æå‡ç³»ç»Ÿæœªæ¥æ•ˆç‡çš„è§„å¾‹ï¼š\n"
                f"> *\"{extracted_insight}\"*\n"
                f"**æ‚¨æ˜¯å¦å…è®¸æˆ‘å°†è¿™æ¡è§„å¾‹åˆ»å…¥ Genesis çš„æ½œæ„è¯†åŸºå› åº“ï¼Ÿ(å›å¤ æ˜¯/Y æˆ– å¦/N)**"
            )
            response += handshake_msg
            
            # Save the pending insight into the context so the next turn can catch it
            self.context.pending_insight = extracted_insight
                    
        # 3.1 è‡ªé€‚åº”å­¦ä¹  (è§‚å¯Ÿäº¤äº’)
        if self.adaptive_learner:
            # ç®€å•åˆ¤æ–­ç”¨æˆ·åé¦ˆ (è¿™é‡Œå‡è®¾æ²¡æœ‰æ˜¾å¼åé¦ˆï¼Œæˆ–è€…ä» response ä¸­æ¨æ–­? 
            # æš‚æ—¶åªè®°å½• message å’Œ responseï¼Œuser_reaction ç•™ç»™ä¸‹ä¸€è½®? 
            # å®é™…ä¸Šæˆ‘ä»¬éœ€è¦ç‹¬ç«‹çš„ feedback æœºåˆ¶ã€‚è¿™é‡Œå…ˆè®°å½•æœ¬æ¬¡äº¤äº’ã€‚)
            self.adaptive_learner.observe_interaction(
                user_message=user_input,
                assistant_response=response
            )
            optimization_info['adaptive_learning'] = 'observed'
            
        # 3.1.5 Dynamic Profile Extraction (New Phase 7)
        if self.profile_evolution:
            # Fire and forget (or await if fast enough)
            # We treat this as part of the cognitive cycle.
            try:
                new_prefs = await self.profile_evolution.extract_dynamic_preferences(user_input)
                if new_prefs:
                    logger.info(f"ğŸ‘¤ Detected New Preferences: {new_prefs}")
                    optimization_info['new_preferences'] = new_prefs
            except Exception as e:
                logger.warning(f"Preference extraction failed: {e}")

        self.metrics_history.append(final_metrics)
        self.last_metrics = final_metrics # ä¸ºè®°å¿†æ•´åˆæä¾›ä¸Šä¸‹æ–‡
        
        # è§¦å‘å†å²è®°å½•å‹ç¼©
        # è§¦å‘å†å²è®°å½•å‹ç¼©
        if self.context.compression_engine:
            try:
                await self.context.compress_history()
            except Exception as e:
                logger.warning(f"å‹ç¼©å¤±è´¥: {e}")
        
        # 3.2 å…¶å®ƒä¼˜åŒ–å™¨ (ä¿æŒå…¼å®¹)
        if self.enable_optimization:
            if self.tool_optimizer and final_metrics:
                self.tool_optimizer.record_sequence(
                    problem_type,
                    final_metrics.tools_used,
                    final_success,
                    {'tokens': final_metrics.total_tokens, 'time': final_metrics.total_time, 'iterations': final_metrics.iterations}
                )
            
            # ä½¿ç”¨ AdaptiveLearner æ›¿ä»£æ—§çš„ UserProfileEvolution
            # ä½†ä¸ºäº†ä¿æŒ stats æ¥å£å…¼å®¹ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¿ç•™ self.profile_evolution å¼•ç”¨?
            # æš‚æ—¶è®©å®ƒå…±å­˜ï¼Œæˆ–è€…å®Œå…¨æ›¿æ¢ã€‚é‰´äº AdaptiveLearner æ›´å¼ºï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨å®ƒã€‚
            
            # 3.3 è®°å¿†æ•´åˆ (Consolidation) - Delegate to Cognition
            if final_success:
                 await self.cognition.consolidate_memory(user_input, response, final_metrics)
        
        # â•â•â• åœ°åŸºå±‚ï¼šä¼šè¯æŒä¹…åŒ– (Session Persistence) â•â•â•
        # Fix Amnesia: Save turn to SQLite SessionManager
        tools_list = final_metrics.tools_used if final_metrics else []
        await self.session_manager.save_turn(user_input, response, tools_list)
        
        # â•â•â• è‡ªé€‚åº”å­¦ä¹ ï¼šè®°å½•äº¤äº’ + æŒ‰éœ€è§¦å‘ LLM åæ€ â•â•â•
        try:
            adaptive = self.optimization_components.get('adaptive_learner') if hasattr(self, 'optimization_components') else None
            if adaptive:
                adaptive.observe_interaction(
                    user_message=user_input,
                    assistant_response=response[:400],
                )
                # éé˜»å¡è§¦å‘åæ€ï¼ˆæ¯ N æ¬¡äº¤äº’æ‰§è¡Œä¸€æ¬¡ LLM è‡ªåæ€è°ƒç”¨ï¼‰
                if adaptive.should_reflect():
                    asyncio.create_task(
                        adaptive.trigger_reflection(llm_chat_fn=self.cognition.chat)
                    )
        except Exception as _al_err:
            logger.debug(f"AdaptiveLearner è·³è¿‡: {_al_err}")
        
        # 4. é”šç‚¹ä¿¡ä»»: æ ‡è®°å“åº”æ¥æº (Anchored Trust)
        tagged_response = response
        if final_metrics and final_metrics.tools_used:
            # Check if any external tools were used
            external_tools = ['web_search', 'browser']
            used_external = any(t.lower() in external_tools for t in final_metrics.tools_used)
            if used_external:
                from genesis.core.trust_anchor import TrustLevel
                disclaimer = self.trust_anchor.get_disclaimer(TrustLevel.L3_EXTERNAL)
                if disclaimer:
                    tagged_response = f"{response}\n\n{disclaimer}"
            
        return {
            'response': tagged_response,
            'metrics': final_metrics,
            'success': final_success,
            'optimization_info': optimization_info
        }

    async def autonomous_step(self, active_mission: Any) -> Dict[str, Any]:
        """
        Execute a single autonomous step for the daemon.
        Bypasses Awareness Phase (Intent is known).
        Uses Metacognition (Entropy) to prevent loops.
        """
        import time
        import os
        from genesis.core.base import Message, MessageRole
        # from genesis.core.entropy import EntropyMonitor # Already imported
        
        logger.info(f"ğŸ¤– Autonomous Step for Mission: {active_mission.objective}")
        
        # 0. Setup
        start_time = time.time()
        cwd = os.getcwd()
        
        # 1. World Model Scan
        from genesis.core.capability import CapabilityScanner
        world_model_snapshot = CapabilityScanner.scan()
        
        # 2. Entropy Check
        # Lazy init monitor if not present (simulating process() behavior)
        if not hasattr(self, 'entropy_monitor'):
             from genesis.core.entropy import EntropyMonitor
             self.entropy_monitor = EntropyMonitor(window_size=6)
        else:
             self.entropy_monitor.reset()  # æ–°è¯·æ±‚ï¼Œé‡ç½®ç†µå†å²

        # Capture previous state (if any) - actually we capture AFTER execution usually, 
        # but here we capture the 'before' state or result of 'previous' step.
        # Let's rely on standard capture at end of loop.
        
        entropy_analysis = self.entropy_monitor.analyze_entropy()
        if entropy_analysis.get('status') == 'stagnant':
            logger.warning(f"âš ï¸ Guardian: High Entropy Detected: {entropy_analysis}")
            
        # 3. Strategy Phase (The Brain)
        # We need to construct a 'user_input' that represents the current state/goal
        # Since there is no user, the 'problem' is the Mission Objective + Last Status
        
        last_out = "None"
        if self.context.get_history_length() > 0:
             last_out = str(self.context._message_history[-1].content)[:200]
             
        current_input = f"Mission: {active_mission.objective}. Status: {active_mission.status}. Last Output: {last_out}"
        
        # Retrieve Active Jobs for context
        active_jobs = []
        if hasattr(self, 'tools'):
            shell_tool = self.tools.get('shell')
            if shell_tool and hasattr(shell_tool, 'job_manager'):
                active_jobs = shell_tool.job_manager.list_jobs(active_only=True)

        # Build History Text manually
        history_txt = ""
        if self.context.get_history_length() > 0:
             recent = self.context._message_history[-10:]
             for m in recent:
                 role = "User" if m.role == MessageRole.USER else "Assistant"
                 history_txt += f"{role}: {str(m.content)[:100]}\n"

        strategic_plan = await self.cognition.strategy_phase(
            user_input=current_input,
            oracle_output={"core_intent": active_mission.objective}, # Mock Oracle
            user_context=history_txt,
            active_mission=active_mission,
            mission_lineage=[], # Could fetch if needed
            world_model=world_model_snapshot,
            active_jobs=active_jobs,
            entropy_analysis=entropy_analysis
        )
        
        # 4. Execution (Single Turn)
        # We use a simplified loop or just call the LLM directly?
        # We need to ACT (Helper function or direct LLM call?)
        # Let's use the core loop mechanism but restricted to 1 turn if possible, 
        # or just reuse the 'chat' capability.
        # But 'process' does a lot of wiring. 
        # For robustness, we'll manually execute the "Thought -> Act" cycle here.
        
        # 4.1 Ask LLM for Action
        # We supplement the strategic plan into the system prompt or history?
        # Strategy Phase returns a "System Prompt" extension or "Thought"?
        # Actually strategy_phase returns a STRING response from the Strategist Persona.
        # We treat that as the 'Plan'.
        
        prompt = f"""
        [AUTONOMOUS MODE]
        Objective: {active_mission.objective}
        Strategic Plan: {strategic_plan}
        
        Execute the next logical step. Use tools if necessary.
        """
        
        # 4.2 Call LLM (using context pipeline)
        # We leverage context.build_messages to get System Prompt + History + Our Prompt
        messages = await self.context.build_messages(user_input=prompt)
        
        # Add "Active Jobs" context manually if needed, or rely on strategy_plan
        # created above which already included it? 
        # The prompt includes 'strategic_plan' which saw active_jobs.
        
        try:
            # Need to convert internal Messages to Dicts for provider
            provider_messages = []
            for i, msg in enumerate(messages):
                msg_dict = msg.to_dict()
                
                if msg_dict["role"] == "tool":
                    prev_msg = provider_messages[-1] if provider_messages else None
                    is_orphan = True
                    if prev_msg and prev_msg.get("role") == "assistant" and prev_msg.get("tool_calls"):
                        target_id = msg_dict.get("tool_call_id")
                        if target_id and any(tc.get("id") == target_id for tc in prev_msg.get("tool_calls", [])):
                            is_orphan = False
                            
                    if is_orphan:
                        logger.debug(f"Sanitizing orphaned tool payload {msg_dict.get('tool_call_id', 'unknown')} -> user")
                        msg_dict["role"] = "user"
                        msg_dict["content"] = f"[System Observation (Tool Result)]:\n{msg_dict.get('content', '')}"
                        if "tool_call_id" in msg_dict: del msg_dict["tool_call_id"]
                        if "name" in msg_dict: del msg_dict["name"]
                        
                provider_messages.append(msg_dict)
            
            response = await self.provider_router.chat_with_failover(
                messages=provider_messages,
                tools=self.tools.get_definitions(),
                model=None
            )
            
            # 4.3 Handle Response (Tool Call or Text)
            # Log raw response
            self.context.add_to_history(Message(role=MessageRole.ASSISTANT, content=response.content or ""))
            
            # Handle Tools
            tool_outputs = []
            if getattr(response, 'tool_calls', None):
                for tool_call in response.tool_calls:
                    # ToolCall object from base.py is flat: id, name, arguments
                    t_name = tool_call.name
                    t_args = tool_call.arguments
                    if isinstance(t_args, str):
                        import json
                        try: t_args = json.loads(t_args)
                        except: pass
                        
                    logger.info(f"ğŸ› ï¸ Guardian Executing: {t_name} {t_args}")
                    
                    tool_instance = self.tools.get(t_name)
                    if tool_instance:
                        try:
                            result = await tool_instance.execute(**t_args)
                        except Exception as e:
                            result = f"Error: {e}"
                    else:
                        result = f"Error: Tool {t_name} not found"
                        
                    tool_outputs.append(f"Tool {t_name} result: {result}")
                    
                    # Add to history
                    self.context.add_to_history(Message(role=MessageRole.TOOL, content=str(result), tool_call_id=tool_call.id))
                    
                    # Capture Entropy
                    self.entropy_monitor.capture(str(result), cwd, active_mission.id)
            
            return {
                "status": "success",
                "output": response.content,
                "tools_executed": len(tool_outputs)
            }
            
        except Exception as e:
            logger.error(f"Guardian Step Failed: {e}")
            return {"status": "error", "error": str(e)}

    # Removed _awareness_phase, _strategy_phase, _consolidate_memory logic as they are now in CognitiveProcessor


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # åœ°åŸºå±‚ï¼šå¯¹è¯æŒä¹…åŒ– (Conversation Persistence)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # End of Agent
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # è®¤çŸ¥è‡ªæ²» (Cognitive Autonomy)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




    async def _memory_replay(self):
        """æ·±åº¦è®°å¿†æ•´åˆ - è®°å¿†å›æ”¾ (Memory Replay)
        
        æ¯ N æ¬¡äº¤äº’è§¦å‘ä¸€æ¬¡ï¼Œå›é¡¾æœ€è¿‘çš„ K æ¡è®°å¿†ï¼Œ
        è®© LLM åˆæˆè·¨äº¤äº’çš„å…ƒæ¨¡å¼ (Meta-Pattern)ã€‚
        """
        if not self.memory or len(self.memory.memories) < 5:
            return
            
        # è·å–æœ€è¿‘çš„ K æ¡è®°å¿†
        k = min(10, len(self.memory.memories))
        recent_memories = self.memory.memories[-k:]
        
        # æ„å»ºå›é¡¾ Prompt
        memories_text = ""
        for i, mem in enumerate(recent_memories, 1):
            content = mem.get('content', '')[:200]  # Truncate
            memories_text += f"{i}. {content}...\n"
        
        replay_prompt = f"""
You are analyzing patterns across multiple past interactions.

Recent Memories:
{memories_text}

Task:
1. Identify recurring themes or topics (e.g., "User frequently asks about Docker").
2. Note any contradictions or corrections.
3. Synthesize a high-level "Meta-Pattern" that summarizes what you've learned about the user.

Output JSON only:
{{
    "meta_pattern": "one sentence summary of pattern",
    "themes": ["theme1", "theme2"],
    "user_intent": "inferred high-level goal of user"
}}
"""
        try:
            messages = [{"role": "user", "content": replay_prompt}]
            response = await self.cloud_provider.chat(messages=messages)
            
            import json
            content = response.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
                
            result = json.loads(content)
            
            # Store meta-pattern as high-level memory
            meta_pattern = result.get("meta_pattern", "")
            if meta_pattern:
                metadata = {
                    "type": "meta_pattern",
                    "themes": result.get("themes", []),
                    "user_intent": result.get("user_intent", "")
                }
                await self.memory.add(f"[META-PATTERN] {meta_pattern}", metadata=metadata)
                logger.info(f"ğŸ§  Meta-Pattern Synthesized: {meta_pattern}")
                
        except Exception as e:
            logger.warning(f"Memory Replay å¤±è´¥: {e}")
    
    def _infer_solution_type(self, response: str) -> str:
        """æ¨æ–­è§£å†³æ–¹æ¡ˆç±»å‹"""
        response_lower = response.lower()
        
        if any(word in response_lower for word in ['config', 'yml', 'yaml', 'json', 'toml']):
            return 'config'
        elif any(word in response_lower for word in ['code', 'python', 'def ', 'class ']):
            return 'code'
        else:
            return 'unknown'
    
    def _expand_option_input(self, user_input: str) -> str:
        """
        é€‰é¡¹æ„ŸçŸ¥æ‰©å±•å™¨ (Option Context Expander)

        å½“ç”¨æˆ·è¾“å…¥æ˜¯å•å­—æ¯/æ•°å­—ï¼ˆå¦‚ 'c'ã€'2'ï¼‰æ—¶ï¼Œæ£€æŸ¥æœ€è¿‘çš„ assistant æ¶ˆæ¯
        æ˜¯å¦åŒ…å«ç»“æ„åŒ–é€‰é¡¹èœå•ï¼ˆé€‰é¡¹A / é€‰é¡¹B / **A:** ç­‰æ ¼å¼ï¼‰ã€‚
        å¦‚æœæ˜¯ï¼Œåˆ™åœ¨ user_input å‰é¢æ‹¼å…¥é€‰é¡¹ä¸Šä¸‹æ–‡ï¼Œè®© awareness_phase èƒ½æ­£ç¡®è§£æã€‚

        éé€‰é¡¹å›å¤åœºæ™¯ä¸å—å½±å“ï¼ˆè¿”å›åŸ user_inputï¼‰ã€‚
        """
        stripped = user_input.strip()
        # åªå¯¹æçŸ­çš„å›å¤ï¼ˆ1-2ä¸ªå­—ç¬¦ï¼‰åšæ‰©å±•
        if len(stripped) > 2:
            return user_input

        option_char = stripped.lower()
        is_option_like = (
            (len(option_char) == 1 and option_char in 'abcde12345')
            or option_char in ('a', 'b', 'c', 'd', 'e', '1', '2', '3', '4', '5')
        )
        if not is_option_like:
            return user_input

        # ä» context é‡Œæ‰¾æœ€è¿‘çš„ assistant æ¶ˆæ¯
        last_assistant_msg = ""
        try:
            if hasattr(self, 'context') and self.context:
                history = self.context.get_history()  # è¿”å› Message åˆ—è¡¨
                for msg in reversed(history):
                    role = getattr(msg, 'role', None) or (msg.get('role') if isinstance(msg, dict) else None)
                    content = getattr(msg, 'content', None) or (msg.get('content') if isinstance(msg, dict) else None)
                    if role == 'assistant' and content:
                        last_assistant_msg = content
                        break
        except Exception:
            return user_input

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é€‰é¡¹èœå•å…³é”®è¯
        menu_signals = ['é€‰é¡¹A', 'é€‰é¡¹B', 'é€‰é¡¹C', '**A:', '**B:', '**C:', '**1.', '**2.', '**3.', 'option a', 'option b']
        has_menu = any(sig.lower() in last_assistant_msg.lower() for sig in menu_signals)
        if not has_menu:
            return user_input

        # æ‹¼å…¥ä¸Šä¸‹æ–‡
        expanded = (
            f"[ä¸Šä¸‹æ–‡ï¼šä½ åœ¨ä¸Šä¸€æ¡å›å¤ä¸­ç»™å‡ºäº†ä»¥ä¸‹é€‰é¡¹èœå•]\n"
            f"{last_assistant_msg[:800]}\n\n"
            f"[ç”¨æˆ·å›å¤ï¼š{user_input.strip().upper()}]\n"
            f"ç”¨æˆ·é€‰æ‹©äº†é€‰é¡¹ {user_input.strip().upper()}ï¼Œè¯·æŒ‰è¯¥é€‰é¡¹æ‰§è¡Œã€‚"
        )
        logger.debug(f"ğŸ”§ é€‰é¡¹æ„ŸçŸ¥ï¼šæ‰©å±•è¾“å…¥ '{user_input}' â†’ åŒ…å«èœå•ä¸Šä¸‹æ–‡")
        return expanded

    def _check_and_optimize(self) -> Dict[str, Any]:
        """Check if self-optimization is needed"""

        if not self.enable_optimization:
            return {}
        # For now, just return basic stats or delegate to components
        return {
            "adaptive_learner": "active" if self.adaptive_learner else "inactive",
            "optimization": "enabled"
        }

    def _error_response(self, error_msg: str) -> Dict[str, Any]:
        """Generate standardized error response"""
        logger.error(error_msg)
        return {
            "response": f"âŒ {error_msg}",
            "metrics": None,
            "success": False,
            "optimization_info": {}
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_interactions': len(self.metrics_history),
            'optimization_enabled': self.enable_optimization
        }
        
        if self.metrics_history:
            # Filter out None metrics (from bridged/failed interactions)
            valid_metrics = [m for m in self.metrics_history if m]
            if valid_metrics:
                total_tokens = sum(m.total_tokens for m in valid_metrics)
                total_time = sum(m.total_time for m in valid_metrics)
                success_count = sum(1 for m in valid_metrics if m.success)
                
                stats.update({
                    'avg_tokens': total_tokens / len(valid_metrics),
                    'avg_time': total_time / len(valid_metrics),
                    'success_rate': success_count / len(valid_metrics),
                    'total_tools_used': sum(len(m.tools_used) for m in valid_metrics)
                })
        
        # ä¼˜åŒ–å™¨ç»Ÿè®¡
        if self.enable_optimization:
            if self.prompt_optimizer:
                stats['prompt_optimizer'] = self.prompt_optimizer.get_stats()
            
            if self.behavior_optimizer:
                stats['behavior_optimizer'] = self.behavior_optimizer.get_stats()
            
            if self.tool_optimizer:
                stats['tool_optimizer'] = self.tool_optimizer.get_stats()
            
            if self.profile_evolution:
                stats['user_profile'] = self.profile_evolution.get_stats()
        
        return stats
    
    def get_optimization_report(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        stats = self.get_stats()
        
        lines = [
            "=" * 60,
            "NanoGenesis ä¼˜åŒ–æŠ¥å‘Š",
            "=" * 60,
            f"\næ€»äº¤äº’æ¬¡æ•°: {stats['total_interactions']}",
        ]
        
        if stats['total_interactions'] > 0:
            lines.extend([
                f"å¹³å‡ Token: {stats['avg_tokens']:.0f}",
                f"å¹³å‡è€—æ—¶: {stats['avg_time']:.2f}s",
                f"æˆåŠŸç‡: {stats['success_rate']:.1%}",
            ])
        
        if self.enable_optimization:
            lines.append("\nè‡ªä¼˜åŒ–ç»Ÿè®¡:")
            
            if 'prompt_optimizer' in stats:
                po = stats['prompt_optimizer']
                lines.extend([
                    f"\næç¤ºè¯ä¼˜åŒ–:",
                    f"  - ä¼˜åŒ–æ¬¡æ•°: {po['total_optimizations']}",
                    f"  - é‡‡ç”¨æ¬¡æ•°: {po['adopted_count']}",
                    f"  - å¹³å‡ Token èŠ‚çœ: {po['avg_token_saved']:.1%}",
                ])
            
            if 'behavior_optimizer' in stats:
                bo = stats['behavior_optimizer']
                lines.extend([
                    f"\nè¡Œä¸ºä¼˜åŒ–:",
                    f"  - ç­–ç•¥æ•°é‡: {bo['total_strategies']}",
                    f"  - å¹³å‡æˆåŠŸç‡: {bo['avg_success_rate']:.1%}",
                    f"  - æ€»ä½¿ç”¨æ¬¡æ•°: {bo['total_uses']}",
                ])
            
            if 'tool_optimizer' in stats:
                to = stats['tool_optimizer']
                lines.extend([
                    f"\nå·¥å…·ä¼˜åŒ–:",
                    f"  - é—®é¢˜ç±»å‹: {to['problem_types']}",
                    f"  - æˆåŠŸç‡: {to['success_rate']:.1%}",
                    f"  - ç¼“å­˜æœ€ä¼˜åºåˆ—: {to['cached_optimal']}",
                ])
            
            if 'user_profile' in stats:
                up = stats['user_profile']
                lines.extend([
                    f"\nç”¨æˆ·ç”»åƒ:",
                    f"  - ä¸“ä¸šé¢†åŸŸ: {', '.join(up['expertise']) if up['expertise'] else 'æœªçŸ¥'}",
                    f"  - åå¥½å·¥å…·: {', '.join(up['preferred_tools'][:3]) if up['preferred_tools'] else 'æœªçŸ¥'}",
                ])
        
        lines.append("=" * 60)
        
        return '\n'.join(lines)
    
    def get_reasoning_log(self) -> list:
        """è·å–å†³ç­–æ¨ç†æ—¥å¿— (Decision Transparency)"""
        return self.reasoning_log
    
    def clear_reasoning_log(self):
        """æ¸…ç©ºæ¨ç†æ—¥å¿—"""
        self.reasoning_log = []
    
    def _log_tool_reason(self, tool_name: str, tool_args: dict):
        """è®°å½•å·¥å…·è°ƒç”¨åŸå›  (Callback for AgentLoop)"""
        from datetime import datetime
        entry = {
            "timestamp": datetime.now().isoformat(),
            "tool": tool_name,
            "args_summary": str(tool_args)[:100],  # Truncate for readability
        }
        self.reasoning_log.append(entry)
        logger.debug(f"ğŸ“ Logged tool call: {tool_name}")
