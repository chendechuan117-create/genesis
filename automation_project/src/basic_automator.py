#!/usr/bin/env python3
"""
åŸºç¡€è‡ªåŠ¨åŒ–è„šæœ¬ - æ¨¡æ‹Ÿç‚¹å‡»å’Œå†…å®¹è·å–
"""

import time
import json
import os
from datetime import datetime
from playwright.sync_api import sync_playwright

class BasicAutomator:
    """åŸºç¡€è‡ªåŠ¨åŒ–å™¨"""
    
    def __init__(self, headless=False, slow_mo=100):
        """
        åˆå§‹åŒ–è‡ªåŠ¨åŒ–å™¨
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            slow_mo: æ“ä½œå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ï¼Œæ¨¡æ‹Ÿäººç±»æ“ä½œ
        """
        self.headless = headless
        self.slow_mo = slow_mo
        self.results_dir = "data/results"
        os.makedirs(self.results_dir, exist_ok=True)
        
    def save_result(self, data, filename_prefix="result"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.results_dir}/{filename_prefix}_{timestamp}.json"
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename
    
    def automate_website(self, url, actions):
        """
        è‡ªåŠ¨åŒ–ç½‘ç«™æ“ä½œ
        
        Args:
            url: ç›®æ ‡ç½‘å€
            actions: æ“ä½œåˆ—è¡¨ï¼Œæ¯ä¸ªæ“ä½œæ˜¯å­—å…¸æ ¼å¼
                {
                    "type": "click"/"extract"/"input",
                    "selector": CSSé€‰æ‹©å™¨,
                    "value": è¾“å…¥å€¼ï¼ˆä»…type="input"éœ€è¦ï¼‰,
                    "wait": ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
                }
        
        Returns:
            æå–çš„å†…å®¹åˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–: {url}")
        
        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = p.chromium.launch(
                headless=self.headless,
                slow_mo=self.slow_mo
            )
            
            # åˆ›å»ºé¡µé¢ä¸Šä¸‹æ–‡
            context = browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            
            page = context.new_page()
            
            try:
                # è®¿é—®ç›®æ ‡é¡µé¢
                print(f"ğŸŒ è®¿é—®: {url}")
                page.goto(url, wait_until="networkidle")
                time.sleep(2)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                
                results = []
                
                # æ‰§è¡Œæ¯ä¸ªæ“ä½œ
                for i, action in enumerate(actions):
                    print(f"ğŸ“ æ‰§è¡Œæ“ä½œ {i+1}/{len(actions)}: {action['type']} -> {action.get('selector', 'N/A')}")
                    
                    try:
                        if action["type"] == "click":
                            page.click(action["selector"])
                            print(f"   âœ… ç‚¹å‡»æˆåŠŸ: {action['selector']}")
                            
                        elif action["type"] == "extract":
                            # æå–æ–‡æœ¬å†…å®¹
                            content = page.inner_text(action["selector"])
                            result_item = {
                                "action": action,
                                "content": content.strip(),
                                "timestamp": datetime.now().isoformat()
                            }
                            results.append(result_item)
                            print(f"   ğŸ“„ æå–å†…å®¹: {content[:100]}...")
                            
                        elif action["type"] == "input":
                            page.fill(action["selector"], action["value"])
                            print(f"   âŒ¨ï¸  è¾“å…¥æˆåŠŸ: {action['value']}")
                        
                        # ç­‰å¾…æŒ‡å®šæ—¶é—´
                        wait_time = action.get("wait", 1)
                        if wait_time > 0:
                            time.sleep(wait_time)
                            
                    except Exception as e:
                        print(f"   âŒ æ“ä½œå¤±è´¥: {e}")
                        # æˆªå›¾ä¿å­˜é”™è¯¯
                        screenshot_path = f"logs/error_{datetime.now().strftime('%H%M%S')}.png"
                        page.screenshot(path=screenshot_path)
                        print(f"   ğŸ“¸ é”™è¯¯æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                
                return results
                
            except Exception as e:
                print(f"âŒ è‡ªåŠ¨åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
                return None
                
            finally:
                # å…³é—­æµè§ˆå™¨
                browser.close()
                print("ğŸ”„ æµè§ˆå™¨å·²å…³é—­")

class MaterialCollector:
    """ç´ ææ”¶é›†å™¨"""
    
    def __init__(self, save_dir="data/materials"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
    
    def collect_text(self, text, source="unknown", tags=None):
        """æ”¶é›†æ–‡æœ¬ç´ æ"""
        if not text or len(text.strip()) < 10:
            return None
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.save_dir}/text_{timestamp}.txt"
        
        metadata = {
            "source": source,
            "timestamp": datetime.now().isoformat(),
            "length": len(text),
            "tags": tags or []
        }
        
        content = f"=== å…ƒæ•°æ® ===\n{json.dumps(metadata, ensure_ascii=False, indent=2)}\n\n"
        content += f"=== å†…å®¹ ===\n{text}\n"
        
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        
        print(f"ğŸ“ æ–‡æœ¬ç´ æå·²ä¿å­˜: {filename}")
        return filename
    
    def collect_urls(self, urls, category="general"):
        """æ”¶é›†URLåˆ—è¡¨"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.save_dir}/urls_{category}_{timestamp}.json"
        
        data = {
            "category": category,
            "timestamp": datetime.now().isoformat(),
            "count": len(urls),
            "urls": urls
        }
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ”— URLåˆ—è¡¨å·²ä¿å­˜: {filename} ({len(urls)}ä¸ª)")
        return filename

def demo_automation():
    """æ¼”ç¤ºè‡ªåŠ¨åŒ–åŠŸèƒ½"""
    print("ğŸ¬ å¼€å§‹æ¼”ç¤ºè‡ªåŠ¨åŒ–...")
    
    # åˆ›å»ºè‡ªåŠ¨åŒ–å™¨
    automator = BasicAutomator(headless=False, slow_mo=200)
    collector = MaterialCollector()
    
    # ç¤ºä¾‹ï¼šè®¿é—®ç™¾åº¦å¹¶æœç´¢
    actions = [
        {"type": "extract", "selector": "title", "wait": 1},
        {"type": "input", "selector": "#kw", "value": "è‡ªåŠ¨åŒ–æµ‹è¯•", "wait": 1},
        {"type": "click", "selector": "#su", "wait": 2},
        {"type": "extract", "selector": ".result", "wait": 1}
    ]
    
    # æ‰§è¡Œè‡ªåŠ¨åŒ–
    results = automator.automate_website("https://www.baidu.com", actions)
    
    if results:
        # ä¿å­˜ç»“æœ
        automator.save_result(results, "baidu_search")
        
        # æ”¶é›†ç´ æ
        for result in results:
            if result["action"]["type"] == "extract":
                collector.collect_text(
                    result["content"],
                    source="baidu",
                    tags=["search", "demo"]
                )
    
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    demo_automation()