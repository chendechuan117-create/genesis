"""
NanoGenesis ä¸»ç±» - æ•´åˆæ‰€æœ‰æ ¸å¿ƒç»„ä»¶ (Unified)
"""

import sys
import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List
import logging

sys.path.insert(0, str(Path(__file__).parent))

from core.base import PerformanceMetrics, Message, MessageRole
from core.registry import ToolRegistry
from core.loop import AgentLoop
from core.context import SimpleContextBuilder
from core.memory import QmdMemory
from core.scheduler import AgencyScheduler
from core.provider import LiteLLMProvider, NativeHTTPProvider, LITELLM_AVAILABLE
from core.provider_local import OllamaProvider  # Re-imported for Embeddings only
from core.config import config
from core.trust_anchor import TrustAnchorManager
from tools.chain_next_tool import ChainNextTool

from optimization.prompt_optimizer import PromptOptimizer
from optimization.behavior_optimizer import BehaviorOptimizer
from optimization.tool_optimizer import ToolUsageOptimizer
from optimization.profile_evolution import UserProfileEvolution
from intelligence.adaptive_learner import AdaptiveLearner

logger = logging.getLogger(__name__)


class NanoGenesis:
    """
    NanoGenesis - è‡ªè¿›åŒ–çš„è½»é‡çº§æ™ºèƒ½ Agent (å•è„‘æ¶æ„ç‰ˆ)
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å•è„‘æ¶æ„ (Unified Brain): DeepSeek V3 å…¨æƒæ¥ç®¡
    2. çœ Token - å¤šå±‚ç¼“å­˜ä¼˜åŒ– + æç¤ºè¯è‡ªä¼˜åŒ– (Compression Protocol)
    3. èƒ½å¹²æ´» - å·¥å…· + æ™ºèƒ½è¯Šæ–­ + ç­–ç•¥å­¦ä¹ 
    4. ä¼šè‡ªæˆ‘è¿­ä»£ - å››é‡è‡ªä¼˜åŒ–æœºåˆ¶
    5. å‘é‡è®°å¿† (Vector Memory) - è¯­ä¹‰æ£€ç´¢
    6. æ—¶é—´æ„ŸçŸ¥ (Time Agency) - åå°è°ƒåº¦
    """
    
    def __init__(
        self,
        user_id: str = "default_user",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "deepseek/deepseek-chat",
        gemini_key: Optional[str] = None,
        gemini_url: Optional[str] = None,
        max_iterations: int = 10,
        enable_optimization: bool = True
    ):
        """åˆå§‹åŒ–"""
        self.user_id = user_id
        self.enable_optimization = enable_optimization
        self.config = config
        
        # 1. æ ¸å¿ƒç»„ä»¶åŸºç¡€
        self.tools = ToolRegistry()
        self.context = SimpleContextBuilder()
        
        # è§£æé…ç½®
        final_api_key = api_key or self.config.deepseek_api_key
        
        # 2. äº‘ç«¯å¤§è„‘ (Cloud Brain) - Multi-Provider Support
        self.providers = {}
        self.active_provider_name = 'antigravity' # Default to Antigravity (Gemini 2.5)
        
        # Primary: DeepSeek (NativeHTTP)
        if final_api_key:
            logger.info("åˆå§‹åŒ– Primary Provider: DeepSeek (NativeHTTP)")
            self.providers['deepseek'] = NativeHTTPProvider(
                api_key=final_api_key,
                base_url=base_url,
                default_model=model
            )
        elif LITELLM_AVAILABLE:
            logger.info("åˆå§‹åŒ– Primary Provider: LiteLLM")
            self.providers['deepseek'] = LiteLLMProvider(
                api_key=api_key,
                base_url=base_url,
                default_model=model
            )
        else:
            logger.warning("æ—  Primary Keyï¼Œä½¿ç”¨ MockLLMProvider")
            from core.provider import MockLLMProvider
            self.providers['deepseek'] = MockLLMProvider()

        # Backup: Gemini (via Proxy)
        final_gemini_key = gemini_key or getattr(self.config, 'gemini_api_key', None)
        final_gemini_url = gemini_url or getattr(self.config, 'gemini_base_url', "http://127.0.0.1:8045/v1")
        
        if final_gemini_key:
            logger.info(f"åˆå§‹åŒ– Backup Provider: Gemini (Proxy at {final_gemini_url})")
            self.providers['gemini'] = NativeHTTPProvider(
                api_key=final_gemini_key,
                base_url=final_gemini_url,
                default_model="gemini-1.5-flash" 
            )
        
        # Extension: Antigravity Tools (Surgical Insertion)
        logger.info("åˆå§‹åŒ– Extension Provider: Antigravity Tools")
        self.providers['antigravity'] = NativeHTTPProvider(
            api_key="sk-8bf1cea5032d4ec0bfd421630814bff0",
            base_url="http://127.0.0.1:8045/v1",
            default_model="gemini-2.5-flash"
        )
        
        # Set initial provider
        self.cloud_provider = self.providers[self.active_provider_name]
        
        # æ³¨å…¥ Provider åˆ° ContextBuilder (ç”¨äºå‹ç¼©)
        self.context.set_provider(self.cloud_provider)

        # 3. åˆå§‹åŒ– QMD è®°å¿† (SQLite + Semantic)
        self.memory = QmdMemory()
        
        # 4. åˆå§‹åŒ– Agency è°ƒåº¦å™¨ (Heartbeat)
        self.scheduler = AgencyScheduler(self.tools)
        
        # 5. é»˜è®¤ä½¿ç”¨äº‘ç«¯å¤§è„‘åˆå§‹åŒ– Loop
        self.loop = AgentLoop(
            tools=self.tools,
            context=self.context,
            provider=self.cloud_provider,
            max_iterations=max_iterations
        )
        
        # 5b. Wire up Decision Transparency callback
        self.loop.on_tool_call = self._log_tool_reason
        
        # 6. è‡ªä¼˜åŒ–ç»„ä»¶
        if enable_optimization:
            self.prompt_optimizer = PromptOptimizer(
                provider=self.cloud_provider,
                optimize_interval=50
            )
            self.behavior_optimizer = BehaviorOptimizer(provider=self.cloud_provider)
            self.tool_optimizer = ToolUsageOptimizer()
            
            # Memento Protocol Tool
            self.tools.register(ChainNextTool())
            
            self.profile_evolution = UserProfileEvolution(user_id)
            # åˆå§‹åŒ– AdaptiveLearner (OpenClaw-style)
            self.adaptive_learner = AdaptiveLearner(
                 storage_path=self.config.workspace_root / "data" / "adaptive_learning.json"
            )
        else:
            self.prompt_optimizer = None
            self.behavior_optimizer = None
            self.tool_optimizer = None
            self.profile_evolution = None
            self.adaptive_learner = None
        
        # 7. æ€§èƒ½ç›‘æ§
        self.metrics_history = []
        
        # 8. å†³ç­–é€æ˜åº¦ (Decision Transparency)
        self.reasoning_log: list = []
        
        # 9. é”šç‚¹ä¿¡ä»» (Anchored Trust)
        self.trust_anchor = TrustAnchorManager()
        
        # 8. æ³¨å†Œå·¥å…·
        self._register_tools()
        
        # 9. åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯
        self._initialize_system_prompt()
        
        # 10. åŠ è½½å…ƒè®¤çŸ¥åè®®
        self._load_meta_cognition_protocol()
        
        # 11. åŠ è½½è¿‘æœŸå¯¹è¯è®°å¿† (åœ°åŸºå±‚) â€” é‡å¯åæ¢å¤ä¸Šä¸‹æ–‡
        self._load_recent_conversations()
        
        logger.debug(f"âœ“ NanoGenesis åˆå§‹åŒ–å®Œæˆ (å•è„‘æ¶æ„, ä¼˜åŒ–: {enable_optimization})")

    def _switch_provider(self, target: str):
        """Switch active provider (e.g., deepseek -> gemini)"""
        if target not in self.providers:
            logger.error(f"Cannot switch to unknown provider: {target}")
            return
            
        logger.warning(f"âš ï¸ Switching Provider: {self.active_provider_name} -> {target}")
        self.active_provider_name = target
        self.cloud_provider = self.providers[target]
        self.context.set_provider(self.cloud_provider)
        # Also update loop provider if loop exists
        if hasattr(self, 'loop'):
            self.loop.provider = self.cloud_provider

    async def _chat_with_failover(self, messages: List[Dict], **kwargs) -> Any:
        """Wrapper for chat with auto-failover"""
        try:
            return await self.cloud_provider.chat(messages=messages, **kwargs)
        except Exception as e:
            # If primary failed and we have backup
            # Generic Failover Logic
            available = list(self.providers.keys())
            current = self.active_provider_name
            
            # Determine backup candidate
            backup = None
            if current == 'deepseek':
                # Prefer antigravity, then gemini
                if 'antigravity' in self.providers: backup = 'antigravity'
                elif 'gemini' in self.providers: backup = 'gemini'
            elif current in ['antigravity', 'gemini']:
                if 'deepseek' in self.providers: backup = 'deepseek'
            
            if backup:
                logger.error(f"Provider {current} Failed: {e}. Failover -> {backup}")
                self._switch_provider(backup)
                try:
                    return await self.cloud_provider.chat(messages=messages, **kwargs)
                except Exception as e2:
                    logger.error(f"Backup Provider {backup} also failed: {e2}")
                    raise e2
            else:
                raise e

    def _load_meta_cognition_protocol(self):
        """åŠ è½½å…ƒè®¤çŸ¥åè®® (Pure Metacognition)"""
        try:
            # 1. åŠ è½½è£å†³è€…åè®® (Strategist)
            strategist_path = Path(__file__).parent / "intelligence/prompts/pure_metacognition_protocol.txt"
            if strategist_path.exists():
                with open(strategist_path, "r", encoding="utf-8") as f:
                    self.meta_protocol = f.read()
                logger.info("âœ“ å·²åŠ è½½è£å†³è€…åè®® (Strategist Protocol)")
            else:
                self.meta_protocol = None
                logger.warning("è£å†³è€…åè®®æ–‡ä»¶ä¸å­˜åœ¨")

            # 2. åŠ è½½æ´å¯Ÿè€…åè®® (Oracle / Intent Recognition)
            oracle_path = Path(__file__).parent / "intelligence/prompts/intent_recognition.txt"
            if oracle_path.exists():
                with open(oracle_path, "r", encoding="utf-8") as f:
                    self.intent_prompt = f.read()
                logger.info("âœ“ å·²åŠ è½½æ´å¯Ÿè€…åè®® (Oracle Protocol)")
            else:
                self.intent_prompt = None
                logger.warning("æ´å¯Ÿè€…åè®®æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            self.meta_protocol = None
            self.intent_prompt = None
            logger.warning(f"åŠ è½½å…ƒè®¤çŸ¥åè®®å¤±è´¥: {e}")

    def _register_tools(self):
        """æ³¨å†Œæ‰€æœ‰å·¥å…·"""
        try:
            from tools.file_tools import (
                ReadFileTool, WriteFileTool,
                AppendFileTool, ListDirectoryTool
            )
            from tools.shell_tool import ShellTool
            from tools.web_tool import WebSearchTool
            from tools.browser_tool import BrowserTool
            from tools.memory_tool import MemoryTool
            from tools.skill_creator_tool import SkillCreatorTool
            from tools.scheduler_tool import SchedulerTool
            
            self.tools.register(ReadFileTool())
            self.tools.register(WriteFileTool())
            self.tools.register(AppendFileTool())
            self.tools.register(ListDirectoryTool())
            self.tools.register(ShellTool(use_sandbox=False))  # Disable Sandbox to allow Host interaction
            self.tools.register(WebSearchTool())
            self.tools.register(BrowserTool())
            self.tools.register(MemoryTool(self.memory))
            self.tools.register(SkillCreatorTool(self.tools))  # Register SkillCreator
            self.tools.register(SchedulerTool(self.scheduler)) # Register SchedulerTool
            
            logger.info(f"âœ“ å·²æ³¨å†Œ {len(self.tools)} ä¸ªå·¥å…·")
        except Exception as e:
            logger.warning(f"æ³¨å†Œå·¥å…·æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _initialize_system_prompt(self):
        """åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯"""
        # 1. åŸºç¡€: ç”¨æˆ·ç”»åƒç”Ÿæˆ
        if self.enable_optimization and self.profile_evolution:
            adaptive_prompt = self.profile_evolution.generate_adaptive_prompt()
            self.context.update_system_prompt(adaptive_prompt)
            
        # 2. è¿›é˜¶: åŠ è½½å†å²ä¼˜åŒ–ç»“æœ (è¦†ç›–ç”»åƒç”Ÿæˆçš„é»˜è®¤Prompt)
        # è¿™ç¡®ä¿äº†"åƒä¸€å ‘é•¿ä¸€æ™º"â€”â€”å¦‚æœç³»ç»Ÿä¹‹å‰è‡ªæˆ‘ä¼˜åŒ–è¿‡ï¼Œå°±ä½¿ç”¨è¿›åŒ–åçš„ç‰ˆæœ¬
        if self.enable_optimization and self.prompt_optimizer:
            optimized_prompt = self.prompt_optimizer.get_latest_optimized_prompt()
            if optimized_prompt:
                self.context.update_system_prompt(optimized_prompt)
                logger.info("âœ“ å·²åŠ è½½è‡ªè¿›åŒ–åçš„ç³»ç»Ÿæç¤ºè¯")
        
        # è®°å½•åˆå§‹æç¤ºè¯
        if self.prompt_optimizer:
            self.prompt_optimizer.current_system_prompt = self.context.system_prompt
    
    async def process(
        self,
        user_input: str,
        user_context: Optional[str] = None,
        problem_type: str = "general",
        step_callback: Optional[Any] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆå•è„‘æ¶æ„ + çº¯ç²¹å…ƒè®¤çŸ¥åè®® + æµå¼æ‰§è¡Œï¼‰
        
        Refactored for "Body Swap":
        - ç§»é™¤ [ACQUISITION_PLAN] ç­‰ç¡¬ç¼–ç åˆ†æ”¯
        - å¯ç”¨ AdaptiveLearner è‡ªé€‚åº”
        - å¯ç”¨ Loop ç›´æ¥å·¥å…·è°ƒç”¨
        """
        import time
        process_start_time = time.time()
        # 0. å‡†å¤‡åŸºåº• Prompt (Persona)
        base_prompt = ""
        if self.adaptive_learner:
            base_prompt = self.adaptive_learner.generate_adaptive_prompt()
        else:
            base_prompt = self.context.system_prompt
            
        # --- Genesis Triad Pipeline ---
        
        # --- Ouroboros Loop (The Navigator) ---
        MAX_RETRIES = 3
        MAX_DAISY_CHAIN = 5
        execution_history = []
        final_response = None
        final_metrics = None
        final_success = False
        final_optimization_info = {}

        loop_count = 0
        error_count = 0
        current_input = user_input
        
        while loop_count < MAX_DAISY_CHAIN:
            loop_count += 1
            logger.info(f"ğŸ”„ Ouroboros Loop: Iteration {loop_count} (Errors: {error_count})")
            
            # 1. æ´å¯Ÿé˜¶æ®µ (Awareness Phase) - ä»…åœ¨ç¬¬ä¸€è½®è¿è¡Œ
            if loop_count == 1:
                oracle_output = await self._awareness_phase(current_input)
                self.reasoning_log.append({
                    "timestamp": time.time(),
                    "stage": "AWARENESS",
                    "content": oracle_output
                })
                logger.info(f"âœ“ æ´å¯Ÿå®Œæˆ: {oracle_output.get('core_intent', 'Unknown')}")
            
            # 2. æˆ˜ç•¥é˜¶æ®µ (Strategy Phase)
            # å°†æ‰§è¡Œå†å²æ³¨å…¥æˆ˜ç•¥ä¸Šä¸‹æ–‡
            current_context = user_context or ""
            if execution_history:
                current_context += f"\n\n[Previous Execution Failures]:\n{json.dumps(execution_history, indent=2, ensure_ascii=False)}"
            
            strategic_blueprint = await self._strategy_phase(current_input, oracle_output, current_context)
            self.reasoning_log.append({
                "timestamp": time.time(),
                "stage": "STRATEGY",
                "content": strategic_blueprint
            })
            
            # --- Clarification Protocol (The Short Circuit) ---
            if "[CLARIFICATION_REQUIRED]" in strategic_blueprint:
                logger.info("âš ï¸ æˆ˜ç•¥é˜¶æ®µè¯·æ±‚æ¾„æ¸…ï¼Œä¸­æ–­æ‰§è¡Œ")
                return {
                    'response': accumulated_response + ("\n\n" if accumulated_response else "") + strategic_blueprint, 
                    'metrics': None,
                    'success': True, 
                    'optimization_info': {'status': 'clarification_requested'}
                }

            logger.info("âœ“ æˆ˜ç•¥è“å›¾å·²ç”Ÿæˆ")
            
            # 3. æ‰§è¡Œé˜¶æ®µ (Execution Phase)
            self.context.update_system_prompt(f"{base_prompt}\n\n{strategic_blueprint}")
            self.loop.provider = self.cloud_provider
            
            try:
                # è®© Loop å¤„ç†æ€è€ƒã€è°ƒç”¨å·¥å…·ã€å†æ€è€ƒã€æœ€ç»ˆå›å¤
                response, metrics = await self.loop.run(
                    user_input=current_input,
                    step_callback=step_callback,
                    user_context=user_context,
                    raw_memory=oracle_output.get("memory_pull", []),
                    **kwargs
                )
                
                final_metrics = metrics
                final_success = metrics.success
                final_optimization_info = self._check_and_optimize() if self.enable_optimization else {}
                
                if metrics.success:
                    # Check for Daisy Chain (Memento Protocol) - Tool Based
                    next_instruction = None
                    
                    # 1. Check Tool Calls (Preferred)
                    if metrics.tool_calls:
                        for tc in metrics.tool_calls:
                            if tc['function']['name'] == 'chain_next':
                                try:
                                    args = json.loads(tc['function']['arguments'])
                                    next_instruction = args.get('instruction')
                                    logger.info(f"ğŸ”— Daisy Chain Triggered via Tool: {next_instruction}")
                                    break
                                except:
                                    pass
                    
                    # 2. Check Text Regex (Legacy Fallback)
                    if not next_instruction and ">> NEXT:" in response:
                        import re
                        match = re.search(r">> NEXT:\s*(.+)", response)
                        if match:
                            next_instruction = match.group(1).strip()
                            logger.info(f"ğŸ”— Daisy Chain Triggered via Text: {next_instruction}")

                    if next_instruction:
                        # Append to accumulator
                        accumulated_response += response + "\n\n---\n\n"
                        
                        # Update Input for next loop
                        current_input = next_instruction
                        execution_history = [] # Clear history as previous step succeeded
                        error_count = 0 # Reset error count for new task
                        continue
                    
                    # Mission Accomplished (No chain)
                    accumulated_response += response
                    logger.info("âœ“ Ouroboros Loop: Mission Accomplished")
                    break
                    
                else:
                    error_count += 1
                    logger.warning(f"âš ï¸ Ouroboros Loop: Error Attempt {error_count} Failed.")
                    error_entry = f"Attempt {error_count} Failed. Last output: {response[-200:] if response else 'No response'}"
                    execution_history.append(error_entry)
                    
                    if error_count >= MAX_RETRIES:
                        accumulated_response += response or "Error: Max retries exceeded."
                        logger.error("âŒ Ouroboros Loop: Max Retries Exceeded.")
                        break
            
            except Exception as e:
                logger.error(f"å¤§è„‘æ‰§è¡Œä¸¥é‡å¤±è´¥: {e}")
                execution_history.append(str(e))
                error_count += 1
                if error_count >= MAX_RETRIES:
                    return self._error_response(f"System Critical: Cloud Brain Failure - {e}")

        # --- Memory Update: Append current turn to session context ---
        # å¿…é¡»æ‰‹åŠ¨å›å†™åˆ° contextï¼Œå¦åˆ™ä¸‹ä¸€è½®å¯¹è¯ä¼šä¸¢å¤±ä¸Šä¸‹æ–‡
        self.context.add_to_history(Message(role=MessageRole.USER, content=user_input))
        self.context.add_to_history(Message(role=MessageRole.ASSISTANT, content=response))

        # 3. è®°å½•ä¸å­¦ä¹  (The Evolution)
        optimization_info = {}
        
        # 3.1 è‡ªé€‚åº”å­¦ä¹  (è§‚å¯Ÿäº¤äº’)
        if self.adaptive_learner:
            # ç®€å•åˆ¤æ–­ç”¨æˆ·åé¦ˆ (è¿™é‡Œå‡è®¾æ²¡æœ‰æ˜¾å¼åé¦ˆï¼Œæˆ–è€…ä» response ä¸­æ¨æ–­? 
            # æš‚æ—¶åªè®°å½• message å’Œ responseï¼Œuser_reaction ç•™ç»™ä¸‹ä¸€è½®? 
            # å®é™…ä¸Šæˆ‘ä»¬éœ€è¦ç‹¬ç«‹çš„ feedback æœºåˆ¶ã€‚è¿™é‡Œå…ˆè®°å½•æœ¬æ¬¡äº¤äº’ã€‚)
            self.adaptive_learner.observe_interaction(
                user_message=user_input,
                assistant_response=response
            )
            optimization_info['adaptive_learning'] = 'observed'

        self.metrics_history.append(metrics)
        self.last_metrics = metrics # ä¸ºè®°å¿†æ•´åˆæä¾›ä¸Šä¸‹æ–‡
        
        # è§¦å‘å†å²è®°å½•å‹ç¼©
        if self.context.compression_engine:
            await self.context.compress_history()
        
        # 3.2 å…¶å®ƒä¼˜åŒ–å™¨ (ä¿æŒå…¼å®¹)
        if self.enable_optimization:
            if self.tool_optimizer:
                self.tool_optimizer.record_sequence(
                    problem_type,
                    metrics.tools_used,
                    success,
                    {'tokens': metrics.total_tokens, 'time': metrics.total_time, 'iterations': metrics.iterations}
                )
            
            # ä½¿ç”¨ AdaptiveLearner æ›¿ä»£æ—§çš„ UserProfileEvolution
            # ä½†ä¸ºäº†ä¿æŒ stats æ¥å£å…¼å®¹ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¿ç•™ self.profile_evolution å¼•ç”¨?
            # æš‚æ—¶è®©å®ƒå…±å­˜ï¼Œæˆ–è€…å®Œå…¨æ›¿æ¢ã€‚é‰´äº AdaptiveLearner æ›´å¼ºï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨å®ƒã€‚
            
            # 3.3 è®°å¿†æ•´åˆ (Consolidation)
            if success:
                # å¼‚æ­¥æˆ–åŒæ­¥æ‰§è¡Œ? ç”±äºæ˜¯æœ€åä¸€æ­¥ï¼ŒåŒæ­¥awaitå³å¯
                await self._consolidate_memory(user_input, response)
        
        # â•â•â• åœ°åŸºå±‚ï¼šæ— æ¡ä»¶æŒä¹…åŒ–å¯¹è¯ â•â•â•
        # ä¸ç®¡è¯„åˆ†é«˜ä½ï¼Œæ¯æ¬¡å¯¹è¯éƒ½å®Œæ•´ä¿å­˜
        await self._persist_conversation(user_input, response, metrics)
        
        # 4. é”šç‚¹ä¿¡ä»»: æ ‡è®°å“åº”æ¥æº (Anchored Trust)
        tagged_response = response
        if metrics and metrics.tools_used:
            # Check if any external tools were used
            external_tools = ['web_search', 'browser']
            used_external = any(t.lower() in external_tools for t in metrics.tools_used)
            if used_external:
                from core.trust_anchor import TrustLevel
                disclaimer = self.trust_anchor.get_disclaimer(TrustLevel.L3_EXTERNAL)
                if disclaimer:
                    tagged_response = f"{response}\n\n{disclaimer}"
            
        return {
            'response': tagged_response,
            'metrics': metrics,
            'success': success,
            'optimization_info': optimization_info
        }

    async def _awareness_phase(self, user_input: str) -> Dict[str, Any]:
        """ç¬¬ä¸€äººæ ¼ï¼šæ´å¯Ÿè€… (The Oracle) - æ„å›¾è¯†åˆ«ä¸èµ„æºæ‰«æ"""
        if not self.intent_prompt:
            return {"core_intent": "General Request", "problem_type": "general", "resource_map": {}}
            
        # å¡«å……æ¨¡æ¿
        prompt = self.intent_prompt.replace("{{user_input}}", user_input)
        
        try:
            # è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§è°ƒç”¨ï¼Œä¸åš ReAct å¾ªç¯
            response = await self._chat_with_failover(
                messages=[{"role": "user", "content": prompt}],
                # å°½é‡è®©è¾“å‡ºä¸º JSON
                response_format={"type": "json_object"} if "json" in prompt.lower() else None
            )
            
            # è§£æ JSON è¾“å‡º
            import json
            content = response.content
            # å¤„ç†å¯èƒ½çš„ markdown å—
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
                
            result = json.loads(content)
            
            # --- QMD Memory Enrichment ---
            memory_hits = []
            if result.get("memory_keywords"):
                keywords = result["memory_keywords"]
                # æ‰§è¡Œè®°å¿†æ‹‰å–
                queries = keywords if isinstance(keywords, list) else [keywords]
                for q in queries:
                    hits = await self.memory.search(q, limit=3)
                    memory_hits.extend(hits)
            
            # å»é‡å¹¶æ’åº
            unique_hits = {h['path']: h for h in memory_hits}.values()
            result["memory_pull"] = sorted(unique_hits, key=lambda x: x['score'], reverse=True)[:5]
            
            # --- Decision Cache Enrichment ---
            decision_hits = []
            for q in (queries if 'queries' in locals() else [user_input]):
                 d_hits = await self.memory.search(q, limit=2, collection="_decisions")
                 decision_hits.extend(d_hits)
            
            unique_decisions = {h['path']: h for h in decision_hits}.values()
            formatted_decisions = []
            for d in unique_decisions:
                import json
                meta = json.loads(d.get('metadata', '{}')) if isinstance(d.get('metadata'), str) else d.get('metadata', {})
                formatted_decisions.append({
                    "insight": meta.get("insight", ""),
                    "outcome": meta.get("outcome", ""),
                    "action": meta.get("action", "")
                })
            result["decision_history"] = formatted_decisions[:3]
            
            return result
        except Exception as e:
            logger.warning(f"æ´å¯Ÿé˜¶æ®µå¤±è´¥: {e}")
            return {"core_intent": user_input[:30], "problem_type": "general", "resource_map": {}}

    async def _strategy_phase(self, user_input: str, oracle_output: Dict[str, Any], user_context: str = None) -> str:
        """ç¬¬äºŒäººæ ¼ï¼šè£å†³è€… (The Strategist) - å…ƒè®¤çŸ¥æˆ˜ç•¥åˆ¶å®š"""
        if not self.meta_protocol:
            return ""
            
        # å¡«å……æ¨¡æ¿
        tools_desc = []
        for tool in self.tools.get_definitions():
            tools_desc.append(f"- {tool['function']['name']}: {tool['function']['description']}")
        tools_str = "\n".join(tools_desc)
        
        # æ ¼å¼åŒ–å†³ç­–ç»éªŒ
        decisions = oracle_output.get("decision_history", [])
        dec_str = ""
        if decisions:
            dec_str = "å†å²ç›¸ä¼¼å†³ç­–å‚è€ƒï¼š\n"
            for d in decisions:
                dec_str += f"- [ç»éªŒ] {d.get('insight')} (ç»“æœ: {d.get('outcome')})\n"
        else:
            dec_str = "æš‚æ— ç›¸å…³å†å²å†³ç­–ç»éªŒã€‚"

        protocol_filled = self.meta_protocol.replace("{{oracle_output}}", str(oracle_output))\
                                          .replace("{{problem}}", user_input)\
                                          .replace("{{context}}", user_context or "")\
                                          .replace("{{user_profile}}", str(self.adaptive_learner.get_stats() if self.adaptive_learner else {}))\
                                          .replace("{{decision_experience}}", dec_str)\
                                          .replace("{{tools}}", tools_str)
        
        try:
            # æˆ˜ç•¥åˆ¶å®šè°ƒç”¨
            response = await self._chat_with_failover(
                messages=[{"role": "user", "content": protocol_filled}]
            )
            return response.content
        except Exception as e:
            logger.warning(f"æˆ˜ç•¥é˜¶æ®µå¤±è´¥: {e}")
            return "Proceed with caution and follow standard ReAct instructions."

    def _error_response(self, msg: str) -> Dict[str, Any]:
        return {
            'response': f"Error: {msg}",
            'metrics': None,
            'success': False,
            'optimization_info': {}
        }
    
    async def _check_and_optimize(self) -> Dict[str, Any]:
        """æ£€æŸ¥å¹¶æ‰§è¡Œä¼˜åŒ–"""
        optimization_info = {}
        
        # 1. æç¤ºè¯ä¼˜åŒ–
        if self.prompt_optimizer and self.prompt_optimizer.should_optimize():
            result = await self.prompt_optimizer.optimize(self.context.system_prompt)
            
            if result and result.adopted:
                # é‡‡ç”¨æ–°æç¤ºè¯
                self.context.update_system_prompt(result.new_prompt)
                self.prompt_optimizer.current_system_prompt = result.new_prompt
                
                optimization_info['prompt_optimized'] = {
                    'token_saved': result.improvement['token_saved'],
                    'reason': result.reason
                }
                
                logger.info(f"âœ“ æç¤ºè¯å·²ä¼˜åŒ–: {result.reason}")
        
        # 2. è¡Œä¸ºä¼˜åŒ–ï¼ˆç­–ç•¥åº“ä¼˜åŒ–ï¼‰
        if self.behavior_optimizer:
            if len(self.behavior_optimizer.strategies) > 0 and \
               len(self.behavior_optimizer.strategies) % 20 == 0:
                self.behavior_optimizer.optimize_strategies()
                optimization_info['strategies_optimized'] = True
        
        # 3. ç”¨æˆ·ç”»åƒè¿›åŒ–
        if self.profile_evolution:
            changes = self.profile_evolution.evolve()
            if changes:
                # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
                new_prompt = self.profile_evolution.generate_adaptive_prompt()
                self.context.update_system_prompt(new_prompt)
                
                optimization_info['profile_evolved'] = changes
                logger.info(f"âœ“ ç”¨æˆ·ç”»åƒå·²è¿›åŒ–: {list(changes.keys())}")
        
        return optimization_info

    async def _consolidate_memory(self, user_input: str, response: str):
        """è®°å¿†æ•´åˆåè®® (Memory Consolidation Protocol)"""
        if not self.memory:
            return

        # 1. å¿«é€Ÿè¿‡æ»¤ (Heuristic)
        if len(user_input) < 5 or len(response) < 10:
            return
            
        # 2. å…ƒè¯„ä»·ä¸å†³ç­–æå– (Meta-Evaluation & Decision Extraction)
        eval_prompt = f"""
Analyze the following interaction to extract a "Decision Event" (AX-Pair).

User Query: {user_input}
Assistant Response: {response}

Tasks:
1. Rate Significance (0-10) based on complexity and novelty.
2. If Score >= 7, define the Decision Event:
   - Situation (S): The core problem and system context.
   - Action (A): The specific solution path or tool sequence chosen.
   - Outcome (R): 'success', 'fail', or 'partial'.
   - Insight: The key pattern or rule learned for future similar situations.

Output JSON only:
{{
    "score": int,
    "reason": "short explanation",
    "decision": {{
        "situation": "S",
        "action": "A",
        "outcome": "success/fail/partial",
        "insight": "key learned pattern"
    }}
}}
"""
        try:
            # ä½¿ç”¨ provider è°ƒç”¨ LLM
            messages = [{"role": "user", "content": eval_prompt}]
            eval_response = await self._chat_with_failover(messages=messages)
            
            # è§£æ JSON
            import json
            content = eval_response.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
                
            result = json.loads(content)
            
            if result.get("score", 0) >= 7:
                # 3. å­˜å…¥å†³ç­–ç¼“å­˜ (Minimalist Decision Cache)
                dec = result.get("decision", {})
                if dec:
                    await self.memory.add_decision(
                        situation=dec.get("situation", user_input),
                        action=dec.get("action", ""),
                        outcome=dec.get("outcome", "success"),
                        insight=dec.get("insight", ""),
                        cost={"tokens": getattr(self.last_metrics, 'total_tokens', 0)}
                    )
                    logger.info(f"ğŸ§  å†³ç­–æµå½¢å·²æ›´æ–°: {dec.get('insight', 'New Pattern')}")
            else:
                logger.debug(f"ğŸ—‘ï¸ äº¤äº’ä»·å€¼è¾ƒä½ï¼Œè·³è¿‡å†³ç­–ç¼“å­˜ (Score: {result.get('score')})")
                
        except Exception as e:
            logger.warning(f"è®°å¿†æ•´åˆå¤±è´¥: {e}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # åœ°åŸºå±‚ï¼šå¯¹è¯æŒä¹…åŒ– (Conversation Persistence)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def _persist_conversation(self, user_input: str, response: str, metrics=None):
        """æ— æ¡ä»¶æŒä¹…åŒ–æ¯æ¬¡å¯¹è¯ï¼ˆåŒå†™ï¼šSQLite + Markdown æ—¥å¿—ï¼‰"""
        import time as _time
        now = datetime.datetime.now()
        
        # æ„å»ºå¯¹è¯è®°å½•
        tools_info = ""
        if metrics and metrics.tools_used:
            tools_info = f"\nå·¥å…·è°ƒç”¨: {', '.join(metrics.tools_used)}"
        
        conversation_entry = (
            f"## {now.strftime('%H:%M:%S')}\n\n"
            f"**ç”¨æˆ·**: {user_input}\n\n"
            f"**Genesis**: {response[:500]}{'...' if len(response) > 500 else ''}"
            f"{tools_info}\n\n---\n\n"
        )
        
        # === å†™å…¥ 1: Markdown æ—¥å¿—æ–‡ä»¶ ===
        try:
            log_dir = Path.home() / ".nanogenesis" / "conversations"
            log_dir.mkdir(parents=True, exist_ok=True)
            log_file = log_dir / f"{now.strftime('%Y-%m-%d')}.md"
            
            if not log_file.exists():
                header = f"# Genesis å¯¹è¯æ—¥å¿— - {now.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n"
                log_file.write_text(header, encoding='utf-8')
            
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(conversation_entry)
            
            logger.debug(f"ğŸ“ å¯¹è¯å·²å†™å…¥æ—¥å¿—: {log_file.name}")
        except Exception as e:
            logger.warning(f"æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
        
        # === å†™å…¥ 2: QmdMemory (SQLite + è¯­ä¹‰å‘é‡) ===
        if self.memory:
            try:
                # ç”¨è¾ƒçŸ­çš„æ‘˜è¦å­˜å…¥ SQLiteï¼Œä¾¿äºè¯­ä¹‰æœç´¢
                content_for_db = f"ç”¨æˆ·: {user_input}\nGenesis: {response[:300]}{tools_info}"
                path = f"conversations/{now.strftime('%Y-%m-%d/%H%M%S')}"
                await self.memory.add(
                    content=content_for_db,
                    path=path,
                    collection="conversations",
                    title=user_input[:60],
                    metadata={
                        "type": "conversation",
                        "timestamp": now.isoformat(),
                        "tools": metrics.tools_used if metrics else [],
                        "tokens": metrics.total_tokens if metrics else 0
                    }
                )
                logger.debug(f"ğŸ’¾ å¯¹è¯å·²å­˜å…¥ QmdMemory: {path}")
            except Exception as e:
                logger.warning(f"QmdMemory å†™å…¥å¤±è´¥: {e}")

    def _load_recent_conversations(self):
        """å¯åŠ¨æ—¶åŠ è½½è¿‘æœŸå¯¹è¯æ—¥å¿—åˆ°ä¸Šä¸‹æ–‡ï¼ˆåƒ OpenClaw è¯» memory/YYYY-MM-DD.mdï¼‰"""
        log_dir = Path.home() / ".nanogenesis" / "conversations"
        if not log_dir.exists():
            logger.debug("æ— å†å²å¯¹è¯æ—¥å¿—")
            return
        
        # è¯»å–ä»Šå¤© + æ˜¨å¤©çš„æ—¥å¿—
        today = datetime.date.today()
        yesterday = today - datetime.timedelta(days=1)
        
        recent_content = ""
        for d in [yesterday, today]:
            log_file = log_dir / f"{d.strftime('%Y-%m-%d')}.md"
            if log_file.exists():
                try:
                    text = log_file.read_text(encoding='utf-8')
                    # åªå–æœ€è¿‘çš„å¯¹è¯ï¼ˆæœ€å 2000 å­—ç¬¦ï¼‰ï¼Œé¿å… token çˆ†ç‚¸
                    if len(text) > 2000:
                        text = "...ï¼ˆæ—©æœŸå¯¹è¯å·²çœç•¥ï¼‰\n" + text[-2000:]
                    recent_content += text + "\n"
                except Exception as e:
                    logger.warning(f"è¯»å–æ—¥å¿—å¤±è´¥ {log_file}: {e}")
        
        if recent_content:
            # æ³¨å…¥åˆ° context çš„ recent_context å±æ€§
            self.context._recent_conversation_context = recent_content.strip()
            loaded_lines = recent_content.count('\n')
            logger.info(f"ğŸ§  å·²åŠ è½½è¿‘æœŸå¯¹è¯è®°å¿† ({loaded_lines} è¡Œ)")
        else:
            self.context._recent_conversation_context = ""
            logger.debug("æ— è¿‘æœŸå¯¹è¯å¯åŠ è½½")

    async def _memory_replay(self):
        """æ·±åº¦è®°å¿†æ•´åˆ - è®°å¿†å›æ”¾ (Memory Replay)
        
        æ¯ N æ¬¡äº¤äº’è§¦å‘ä¸€æ¬¡ï¼Œå›é¡¾æœ€è¿‘çš„ K æ¡è®°å¿†ï¼Œ
        è®© LLM åˆæˆè·¨äº¤äº’çš„å…ƒæ¨¡å¼ (Meta-Pattern)ã€‚
        """
        if not self.memory or len(self.memory.memories) < 5:
            return
            
        # è·å–æœ€è¿‘çš„ K æ¡è®°å¿†
        k = min(10, len(self.memory.memories))
        recent_memories = self.memory.memories[-k:]
        
        # æ„å»ºå›é¡¾ Prompt
        memories_text = ""
        for i, mem in enumerate(recent_memories, 1):
            content = mem.get('content', '')[:200]  # Truncate
            memories_text += f"{i}. {content}...\n"
        
        replay_prompt = f"""
You are analyzing patterns across multiple past interactions.

Recent Memories:
{memories_text}

Task:
1. Identify recurring themes or topics (e.g., "User frequently asks about Docker").
2. Note any contradictions or corrections.
3. Synthesize a high-level "Meta-Pattern" that summarizes what you've learned about the user.

Output JSON only:
{{
    "meta_pattern": "one sentence summary of pattern",
    "themes": ["theme1", "theme2"],
    "user_intent": "inferred high-level goal of user"
}}
"""
        try:
            messages = [{"role": "user", "content": replay_prompt}]
            response = await self.cloud_provider.chat(messages=messages)
            
            import json
            content = response.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
                
            result = json.loads(content)
            
            # Store meta-pattern as high-level memory
            meta_pattern = result.get("meta_pattern", "")
            if meta_pattern:
                metadata = {
                    "type": "meta_pattern",
                    "themes": result.get("themes", []),
                    "user_intent": result.get("user_intent", "")
                }
                await self.memory.add(f"[META-PATTERN] {meta_pattern}", metadata=metadata)
                logger.info(f"ğŸ§  Meta-Pattern Synthesized: {meta_pattern}")
                
        except Exception as e:
            logger.warning(f"Memory Replay å¤±è´¥: {e}")
    
    def _infer_solution_type(self, response: str) -> str:
        """æ¨æ–­è§£å†³æ–¹æ¡ˆç±»å‹"""
        response_lower = response.lower()
        
        if any(word in response_lower for word in ['config', 'yml', 'yaml', 'json', 'toml']):
            return 'config'
        elif any(word in response_lower for word in ['code', 'python', 'def ', 'class ']):
            return 'code'
        else:
            return 'unknown'
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_interactions': len(self.metrics_history),
            'optimization_enabled': self.enable_optimization
        }
        
        if self.metrics_history:
            total_tokens = sum(m.total_tokens for m in self.metrics_history)
            total_time = sum(m.total_time for m in self.metrics_history)
            success_count = sum(1 for m in self.metrics_history if m.success)
            
            stats.update({
                'avg_tokens': total_tokens / len(self.metrics_history),
                'avg_time': total_time / len(self.metrics_history),
                'success_rate': success_count / len(self.metrics_history),
                'total_tools_used': sum(len(m.tools_used) for m in self.metrics_history)
            })
        
        # ä¼˜åŒ–å™¨ç»Ÿè®¡
        if self.enable_optimization:
            if self.prompt_optimizer:
                stats['prompt_optimizer'] = self.prompt_optimizer.get_stats()
            
            if self.behavior_optimizer:
                stats['behavior_optimizer'] = self.behavior_optimizer.get_stats()
            
            if self.tool_optimizer:
                stats['tool_optimizer'] = self.tool_optimizer.get_stats()
            
            if self.profile_evolution:
                stats['user_profile'] = self.profile_evolution.get_stats()
        
        return stats
    
    def get_optimization_report(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        stats = self.get_stats()
        
        lines = [
            "=" * 60,
            "NanoGenesis ä¼˜åŒ–æŠ¥å‘Š",
            "=" * 60,
            f"\næ€»äº¤äº’æ¬¡æ•°: {stats['total_interactions']}",
        ]
        
        if stats['total_interactions'] > 0:
            lines.extend([
                f"å¹³å‡ Token: {stats['avg_tokens']:.0f}",
                f"å¹³å‡è€—æ—¶: {stats['avg_time']:.2f}s",
                f"æˆåŠŸç‡: {stats['success_rate']:.1%}",
            ])
        
        if self.enable_optimization:
            lines.append("\nè‡ªä¼˜åŒ–ç»Ÿè®¡:")
            
            if 'prompt_optimizer' in stats:
                po = stats['prompt_optimizer']
                lines.extend([
                    f"\næç¤ºè¯ä¼˜åŒ–:",
                    f"  - ä¼˜åŒ–æ¬¡æ•°: {po['total_optimizations']}",
                    f"  - é‡‡ç”¨æ¬¡æ•°: {po['adopted_count']}",
                    f"  - å¹³å‡ Token èŠ‚çœ: {po['avg_token_saved']:.1%}",
                ])
            
            if 'behavior_optimizer' in stats:
                bo = stats['behavior_optimizer']
                lines.extend([
                    f"\nè¡Œä¸ºä¼˜åŒ–:",
                    f"  - ç­–ç•¥æ•°é‡: {bo['total_strategies']}",
                    f"  - å¹³å‡æˆåŠŸç‡: {bo['avg_success_rate']:.1%}",
                    f"  - æ€»ä½¿ç”¨æ¬¡æ•°: {bo['total_uses']}",
                ])
            
            if 'tool_optimizer' in stats:
                to = stats['tool_optimizer']
                lines.extend([
                    f"\nå·¥å…·ä¼˜åŒ–:",
                    f"  - é—®é¢˜ç±»å‹: {to['problem_types']}",
                    f"  - æˆåŠŸç‡: {to['success_rate']:.1%}",
                    f"  - ç¼“å­˜æœ€ä¼˜åºåˆ—: {to['cached_optimal']}",
                ])
            
            if 'user_profile' in stats:
                up = stats['user_profile']
                lines.extend([
                    f"\nç”¨æˆ·ç”»åƒ:",
                    f"  - ä¸“ä¸šé¢†åŸŸ: {', '.join(up['expertise']) if up['expertise'] else 'æœªçŸ¥'}",
                    f"  - åå¥½å·¥å…·: {', '.join(up['preferred_tools'][:3]) if up['preferred_tools'] else 'æœªçŸ¥'}",
                ])
        
        lines.append("=" * 60)
        
        return '\n'.join(lines)
    
    def get_reasoning_log(self) -> list:
        """è·å–å†³ç­–æ¨ç†æ—¥å¿— (Decision Transparency)"""
        return self.reasoning_log
    
    def clear_reasoning_log(self):
        """æ¸…ç©ºæ¨ç†æ—¥å¿—"""
        self.reasoning_log = []
    
    def _log_tool_reason(self, tool_name: str, tool_args: dict):
        """è®°å½•å·¥å…·è°ƒç”¨åŸå›  (Callback for AgentLoop)"""
        from datetime import datetime
        entry = {
            "timestamp": datetime.now().isoformat(),
            "tool": tool_name,
            "args_summary": str(tool_args)[:100],  # Truncate for readability
        }
        self.reasoning_log.append(entry)
        logger.debug(f"ğŸ“ Logged tool call: {tool_name}")
