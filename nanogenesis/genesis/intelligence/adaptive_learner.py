"""
AdaptiveLearner v2 â€” LLM é©±åŠ¨çš„è‡ªåæ€å­¦ä¹ å™¨
=============================================
æ ¸å¿ƒè½¬å˜ï¼š
  v1: ç¡¬ç¼–ç å…³é”®è¯ + å›ºå®š delta â†’ ç»™é£æ ¼å‚æ•°æ‰“åˆ†
  v2: å­˜å‚¨åŸå§‹äº¤äº’ â†’ æ¯ N æ¬¡è§¦å‘ LLM è‡ªåæ€ â†’ ç”Ÿæˆ cognitive_insights

æ²¡æœ‰ç¡¬ç¼–ç çš„å…³é”®è¯ã€é˜ˆå€¼æˆ–è°ƒæ•´å¹…åº¦ã€‚
æ‰€æœ‰è§„å¾‹ç”± LLM è‡ªèº«ä»äº¤äº’å†å²ä¸­å½’çº³ï¼Œå†™å…¥ cognitive_insightsã€‚
cognitive_insights ç›´æ¥æ³¨å…¥ system_promptï¼Œå½¢æˆè¡Œä¸ºæŒ‡å¯¼ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
  learner = AdaptiveLearner(storage_path="...", reflection_interval=5)
  
  # è®°å½•ä¸€æ¬¡äº¤äº’ï¼ˆåŒæ­¥ï¼Œè½»é‡ï¼‰
  learner.observe_interaction(user_message, assistant_response, user_reaction)
  
  # æ¯ N æ¬¡äº¤äº’åï¼Œå¤–éƒ¨è°ƒç”¨è§¦å‘å¼‚æ­¥åæ€ï¼ˆéœ€ä¼ å…¥ LLM chat å‡½æ•°ï¼‰
  await learner.trigger_reflection(llm_chat_fn=cognition.chat)
  
  # ç”Ÿæˆæ³¨å…¥ system_prompt çš„ insight æ®µè½
  prompt_addon = learner.generate_adaptive_prompt()
"""

import json
import asyncio
import logging
from typing import Dict, List, Optional, Any, Callable, Awaitable
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class AdaptiveState:
    """
    ç²¾ç®€åçš„è‡ªé€‚åº”çŠ¶æ€ã€‚
    ä¸å†ç»´æŠ¤æ‰“åˆ†å‚æ•°ï¼Œåªä¿ç•™ï¼š
      - åŸå§‹äº¤äº’è®¡æ•°
      - LLM è‡ªåæ€ç”Ÿæˆçš„ cognitive_insights
    """
    total_interactions: int = 0
    last_reflection_at: int = 0         # ä¸Šæ¬¡åæ€æ—¶çš„ interaction æ•°é‡
    cognitive_insights: List[str] = field(default_factory=list)

    # å‘åå…¼å®¹ï¼šä¿ç•™æ—§å­—æ®µï¼ˆä¸å†ä½¿ç”¨ï¼Œä½†é¿å…åŠ è½½æ—§ JSON æŠ¥é”™ï¼‰
    prefers_concise: float = 0.5
    prefers_technical: float = 0.5
    prefers_proactive: float = 0.5
    uses_emoji: float = 0.0
    message_length_avg: float = 50.0
    formality: float = 0.5
    positive_signals: int = 0
    negative_signals: int = 0
    confidence: float = 0.0


class AdaptiveLearner:
    """
    LLM é©±åŠ¨çš„è‡ªé€‚åº”å­¦ä¹ å™¨ã€‚

    å‚æ•°ï¼š
      storage_path        : JSON çŠ¶æ€æ–‡ä»¶è·¯å¾„
      reflection_interval : æ¯éš”å¤šå°‘æ¬¡äº¤äº’è§¦å‘ä¸€æ¬¡ LLM åæ€
      max_insights        : cognitive_insights æœ€å¤§æ¡æ•°ï¼ˆFIFO æ·˜æ±°ï¼‰
      history_window      : æ¯æ¬¡åæ€å‚è€ƒæœ€è¿‘ N æ¡åŸå§‹äº¤äº’
    """

    def __init__(
        self,
        storage_path: str = "./data/adaptive_learning.json",
        reflection_interval: int = 5,
        max_insights: int = 12,
        history_window: int = 10,
    ):
        self.storage_path = Path(storage_path)
        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        self.reflection_interval = reflection_interval
        self.max_insights = max_insights
        self.history_window = history_window

        self.state = self._load()
        self.interaction_history: List[Dict[str, Any]] = []

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def observe_interaction(
        self,
        user_message: str,
        assistant_response: str,
        user_reaction: Optional[str] = None,
    ) -> None:
        """
        è®°å½•ä¸€æ¬¡äº¤äº’ï¼ˆåŒæ­¥ï¼Œè½»é‡ï¼Œæ—  LLM è°ƒç”¨ï¼‰ã€‚
        ä»…å­˜å‚¨åŸå§‹æ•°æ®ï¼Œä¸åšä»»ä½•ç¡¬ç¼–ç åˆ†æã€‚
        """
        self.interaction_history.append({
            "timestamp": datetime.now().isoformat(),
            "user": user_message[:500],           # æˆªæ–­é˜²æ­¢è¿‡é•¿
            "assistant": assistant_response[:500],
            "reaction": (user_reaction or "")[:200],
        })
        self.state.total_interactions += 1
        self._save()

    def should_reflect(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åˆ°äº†è§¦å‘ LLM åæ€çš„æ—¶æœº"""
        due = self.state.total_interactions - self.state.last_reflection_at
        return due >= self.reflection_interval and len(self.interaction_history) >= 2

    async def trigger_reflection(
        self,
        llm_chat_fn: Callable[[List[Dict]], Awaitable[Any]],
    ) -> None:
        """
        è§¦å‘ LLM è‡ªåæ€ï¼š
          1. å–æœ€è¿‘ N æ¡äº¤äº’å†å²
          2. æ„å»ºåæ€ prompt
          3. è§£æ LLM è¾“å‡ºä¸º insight åˆ—è¡¨
          4. è¿½åŠ åˆ° cognitive_insights (FIFO)

        Args:
            llm_chat_fn: æ¥å— messages listã€è¿”å›æœ‰ .content å±æ€§çš„å¯¹è±¡çš„å¼‚æ­¥å‡½æ•°
                         ï¼ˆä¸ cognition.chat æ¥å£å…¼å®¹ï¼‰
        """
        if not self.should_reflect():
            return

        recent = self.interaction_history[-self.history_window:]
        history_text = self._format_history(recent)

        prompt = (
            f"ä½ åˆšåˆšå®Œæˆäº† {len(recent)} æ¬¡å¯¹è¯äº¤äº’ï¼Œä»¥ä¸‹æ˜¯è®°å½•ï¼š\n\n"
            f"{history_text}\n\n"
            "è¯·ä»ä¸­å½’çº³ 3~5 æ¡ç®€æ´çš„è§„å¾‹ï¼Œæ¶µç›–ï¼š\n"
            "1. è¿™ä¸ªç”¨æˆ·çš„æ²Ÿé€šåå¥½ï¼ˆè¯­æ°”ã€è¯¦ç»†ç¨‹åº¦ã€æŠ€æœ¯æ·±åº¦ç­‰ï¼‰\n"
            "2. å“ªäº›æ‰§è¡Œæ–¹å¼/å·¥å…·/ç­–ç•¥æœ‰æ•ˆï¼Œå“ªäº›éœ€è¦é¿å…\n"
            "3. ä»»ä½•å…¶ä»–å€¼å¾—è®°ä½çš„è¡Œä¸ºæ¨¡å¼\n\n"
            "æ ¼å¼ï¼šæ¯æ¡è§„å¾‹ä¸€è¡Œï¼Œä»¥ - å¼€å¤´ï¼Œç”¨ä¸­æ–‡ï¼Œç®€çŸ­ç²¾ç‚¼ï¼ˆä¸è¶…è¿‡ 25 å­—/æ¡ï¼‰ã€‚\n"
            "ç›´æ¥è¾“å‡ºè§„å¾‹åˆ—è¡¨ï¼Œä¸è¦æœ‰å‰è¨€æˆ–è§£é‡Šã€‚"
        )

        try:
            resp = await llm_chat_fn([{"role": "user", "content": prompt}])
            raw = resp.content.strip() if resp else ""
            insights = self._parse_insights(raw)

            for insight in insights:
                self.add_cognitive_insight(insight)

            self.state.last_reflection_at = self.state.total_interactions
            self._save()
            logger.info(f"ğŸ§  AdaptiveLearner åæ€å®Œæˆï¼Œæ–°å¢ {len(insights)} æ¡ insight (å…± {len(self.state.cognitive_insights)} æ¡)")

        except Exception as e:
            logger.warning(f"AdaptiveLearner åæ€å¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {e}")

    async def trigger_anchor_reflection(
        self,
        llm_chat_fn: Callable[[List[Dict]], Awaitable[Any]],
        decisions: List[Dict],
    ) -> None:
        """
        æ·±åº¦é”šç‚¹åæ€ â€” ä»…åœ¨é”šç‚¹äº‹ä»¶ï¼ˆå›æº¯ã€å¤±è´¥ï¼‰è§¦å‘ã€‚
        
        ä¸¤ä¸ªç»´åº¦ï¼š
          1. è®¤çŸ¥åŸç†æç‚¼ï¼šä»æˆåŠŸ/å¤±è´¥å¯¹æ¯”æç‚¼åŸŸæ— å…³çš„é”šç‚¹é€‰æ‹©åŸç†ï¼ˆä¹˜æ³•ï¼‰
          2. å·¥å…·æ•ˆèƒ½å®¡è®¡ï¼šè®¡ç®—æ¯ä¸ªå·¥å…·çš„æˆåŠŸç‡ï¼Œæ ‡è®°é«˜å¤±è´¥ç‡å·¥å…·ï¼ˆå·¥å…·ç®±ä¼˜åŒ–ï¼‰

        Args:
            llm_chat_fn: å¼‚æ­¥ LLM chat å‡½æ•°
            decisions  : get_recent_decisions() è¿”å›çš„å†³ç­–è®°å½•åˆ—è¡¨
        """
        if not decisions:
            return

        # â”€â”€ 1. æ„å»ºå†³ç­–æ‘˜è¦ï¼ˆç”¨äºè®¤çŸ¥åŸç†æç‚¼ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        lines = []
        for d in decisions[:12]:
            outcome_emoji = "âœ…" if d["outcome"] == "success" else "âŒ"
            opts = ", ".join(d["anchor_options"][:4]) if d["anchor_options"] else "æœªè®°å½•"
            lines.append(
                f"{outcome_emoji} [{d['problem_type']}] å€™é€‰é”šç‚¹: [{opts}] â†’ é€‰æ‹©: {d['chosen_anchor'][:80]}"
            )
        decisions_text = "\n".join(lines)

        # â”€â”€ 2. è®¡ç®—æ¯ä¸ªå·¥å…·çš„æˆåŠŸç‡ï¼ˆç”¨äºå·¥å…·å®¡è®¡ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        from collections import defaultdict
        tool_stats: dict = defaultdict(lambda: {"success": 0, "failed": 0, "backtracked": 0})
        for d in decisions:
            anchor = d["chosen_anchor"][:40].strip()
            outcome = d["outcome"]
            if outcome in tool_stats[anchor]:
                tool_stats[anchor][outcome] += 1
        
        # åªä¿ç•™æœ‰è¶³å¤Ÿæ ·æœ¬ï¼ˆâ‰¥2æ¬¡ï¼‰çš„å·¥å…·ï¼Œå¹¶è®¡ç®—å¤±è´¥ç‡
        audit_lines = []
        for tool, stats in tool_stats.items():
            total = stats["success"] + stats["failed"] + stats["backtracked"]
            if total < 2:
                continue
            fail_rate = (stats["failed"] + stats["backtracked"]) / total
            bar = "âš ï¸" if fail_rate > 0.5 else ("ğŸ”¸" if fail_rate > 0.25 else "âœ…")
            audit_lines.append(
                f"{bar} [{tool}] æˆåŠŸ:{stats['success']} å¤±è´¥:{stats['failed']} å›æº¯:{stats['backtracked']} "
                f"(å¤±è´¥ç‡:{fail_rate:.0%})"
            )
        audit_text = "\n".join(audit_lines) if audit_lines else "ï¼ˆæ ·æœ¬é‡ä¸è¶³ï¼Œæš‚æ— å·¥å…·å®¡è®¡æ•°æ®ï¼‰"

        # â”€â”€ 3. è®¤çŸ¥åŸç†æç‚¼ prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        principle_prompt = (
            "ä»¥ä¸‹æ˜¯æˆ‘æœ€è¿‘çš„å†³ç­–è®°å½•ï¼š\n\n"
            f"{decisions_text}\n\n"
            "è¯·å½’çº³ 2~3 æ¡**ä¸å…·ä½“ä»»åŠ¡æ— å…³**çš„é€šç”¨è®¤çŸ¥åŸç†ï¼ˆå…³äº'å¦‚ä½•é€‰æ‹©æ›´å¥½èµ·ç‚¹'çš„æ€ç»´è§„å¾‹ï¼‰ã€‚\n"
            "- ä¸è¦å†™'éŸ³é¢‘ç”¨PulseAudio'è¿™ç§ç‰¹å®šè§£æ³•\n"
            "- è¦å†™'å¤±è´¥é”šç‚¹é€šå¸¸æ˜¯ä»é›¶æ„å»ºè€Œéå¯»æ‰¾å·²æœ‰è§£'è¿™ç§æ™®é€‚åŸç†\n"
            "æ¯æ¡ä»¥ - å¼€å¤´ï¼Œä¸­æ–‡ï¼Œä¸è¶…è¿‡ 30 å­—ã€‚ç›´æ¥è¾“å‡ºåˆ—è¡¨ã€‚"
        )

        # â”€â”€ 4. å·¥å…·å®¡è®¡ prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        audit_prompt = (
            "ä»¥ä¸‹æ˜¯æˆ‘å„ä¸ªå·¥å…·/æ–¹æ³•çš„è¿‘æœŸæ‰§è¡ŒæˆåŠŸç‡ç»Ÿè®¡ï¼š\n\n"
            f"{audit_text}\n\n"
            "è¯·åŸºäºè¿™ä»½æ•°æ®ï¼Œè¯†åˆ« 1~2 ä¸ªæœ€å€¼å¾—å…³æ³¨çš„é—®é¢˜ï¼Œç”¨ä¸€å¥è¯æŒ‡æ˜ï¼š\n"
            "  â‘  å“ªä¸ªå·¥å…·å¤±è´¥ç‡æœ€é«˜ï¼Œå¯èƒ½éœ€è¦æ”¹å†™æˆ–æ›¿æ¢\n"
            "  â‘¡ æˆ–è€…ä»€ä¹ˆç±»å‹çš„é”šç‚¹é€‰æ‹©æ¨¡å¼é£é™©æœ€é«˜\n"
            "æ¯æ¡ä»¥ - å¼€å¤´ï¼Œä¸­æ–‡ï¼Œä¸è¶…è¿‡ 30 å­—ã€‚ç›´æ¥è¾“å‡ºåˆ—è¡¨ï¼ˆæ²¡æœ‰é—®é¢˜åˆ™å›å¤'æš‚æ— 'ï¼‰ã€‚"
        )

        try:
            # å¹¶è¡Œä¸¤ä¸ªç‹¬ç«‹ promptï¼ˆéƒ½æ˜¯è½»é‡çº§è°ƒç”¨ï¼‰
            import asyncio
            principle_task = asyncio.create_task(
                llm_chat_fn([{"role": "user", "content": principle_prompt}])
            )
            audit_task = asyncio.create_task(
                llm_chat_fn([{"role": "user", "content": audit_prompt}])
            )
            principle_resp, audit_resp = await asyncio.gather(principle_task, audit_task)

            # å­˜å…¥è®¤çŸ¥åŸç†
            principle_raw = principle_resp.content.strip() if principle_resp else ""
            for insight in self._parse_insights(principle_raw):
                self.add_cognitive_insight(f"[é”šç‚¹è®¤çŸ¥] {insight}")

            # å­˜å…¥å·¥å…·å®¡è®¡
            audit_raw = audit_resp.content.strip() if audit_resp else ""
            if audit_raw and audit_raw != "æš‚æ— ":
                for insight in self._parse_insights(audit_raw):
                    self.add_cognitive_insight(f"[å·¥å…·å®¡è®¡] {insight}")

            self._save()
            logger.info(
                f"ğŸ” é”šç‚¹æ·±åº¦åæ€å®Œæˆ (å…± {len(self.state.cognitive_insights)} æ¡ insight)"
            )

        except Exception as e:
            logger.warning(f"é”šç‚¹åæ€å¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {e}")

    def add_cognitive_insight(self, insight: str) -> None:
        """æ‰‹åŠ¨æ·»åŠ ä¸€æ¡ insightï¼ˆä¹Ÿå¯ä»å¤–éƒ¨è°ƒç”¨ï¼Œä¾‹å¦‚ backtrack è§¦å‘æ—¶ï¼‰"""

        insight = insight.strip()
        if not insight or insight in self.state.cognitive_insights:
            return
        self.state.cognitive_insights.append(insight)
        if len(self.state.cognitive_insights) > self.max_insights:
            self.state.cognitive_insights.pop(0)  # FIFO æ·˜æ±°æœ€æ—§çš„
        self._save()

    def generate_adaptive_prompt(self) -> str:
        """
        ç”Ÿæˆæ³¨å…¥ system_prompt çš„è‡ªé€‚åº”æ®µè½ã€‚
        åªæç‚¼ cognitive_insightsï¼Œä¸å†æœ‰ä»»ä½•è¯„åˆ†æˆ–ç¡¬ç¼–ç è¡¨è¾¾ã€‚
        å¦‚æœæ²¡æœ‰ insightï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆä¸å½±å“ç°æœ‰ promptï¼‰ã€‚
        """
        insights = self.state.cognitive_insights
        if not insights:
            return ""

        lines = ["", "ã€ğŸ“– ä»å†å²äº¤äº’å½’çº³çš„è¡Œä¸ºè§„å¾‹ï¼ˆè‡ªåŠ¨å­¦ä¹ ï¼‰ã€‘"]
        for insight in insights[-8:]:   # åªç”¨æœ€è¿‘ 8 æ¡ï¼Œé¿å…è¿‡é•¿
            lines.append(f"- {insight}")
        lines.append("")
        return "\n".join(lines)

    def get_stats(self) -> Dict[str, Any]:
        """è¿”å›å½“å‰å­¦ä¹ çŠ¶æ€ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
        return {
            "total_interactions": self.state.total_interactions,
            "last_reflection_at": self.state.last_reflection_at,
            "insight_count": len(self.state.cognitive_insights),
            "insights": self.state.cognitive_insights,
            "next_reflection_in": max(
                0, self.reflection_interval - (
                    self.state.total_interactions - self.state.last_reflection_at
                )
            ),
        }

    # ------------------------------------------------------------------
    # Internals
    # ------------------------------------------------------------------

    def _format_history(self, interactions: List[Dict]) -> str:
        lines = []
        for i, item in enumerate(interactions, 1):
            lines.append(f"[{i}] ç”¨æˆ·: {item.get('user', '')}")
            lines.append(f"    Genesis: {item.get('assistant', '')}")
            if item.get("reaction"):
                lines.append(f"    ç”¨æˆ·ååº”: {item['reaction']}")
        return "\n".join(lines)

    def _parse_insights(self, raw: str) -> List[str]:
        """ä» LLM è¾“å‡ºä¸­æå–ä»¥ - å¼€å¤´çš„ insight è¡Œ"""
        insights = []
        for line in raw.splitlines():
            line = line.strip()
            if line.startswith("-"):
                clean = line.lstrip("-").strip()
                if clean and len(clean) > 3:
                    insights.append(clean)
        return insights[:6]  # æœ€å¤šå– 6 æ¡é˜²æ­¢è¿‡è½½

    def _save(self) -> None:
        from dataclasses import asdict
        data = {
            "state": asdict(self.state),
            "last_updated": datetime.now().isoformat(),
        }
        try:
            with open(self.storage_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"AdaptiveLearner ä¿å­˜å¤±è´¥: {e}")

    def _load(self) -> AdaptiveState:
        if not self.storage_path.exists():
            return AdaptiveState()
        try:
            with open(self.storage_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆv1 å­˜çš„æ˜¯ pattern å­—æ®µï¼‰
            raw = data.get("state") or data.get("pattern") or {}
            # è¿‡æ»¤æ‰ AdaptiveState ä¸è®¤è¯†çš„å­—æ®µï¼Œé˜²æ­¢ __init__ æŠ¥é”™
            valid_fields = AdaptiveState.__dataclass_fields__.keys()
            filtered = {k: v for k, v in raw.items() if k in valid_fields}
            return AdaptiveState(**filtered)
        except Exception as e:
            logger.warning(f"AdaptiveLearner åŠ è½½å¤±è´¥ï¼Œé‡ç½®: {e}")
            return AdaptiveState()
